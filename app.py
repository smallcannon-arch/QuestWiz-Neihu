import streamlit as st
import google.generativeai as genai
import random
import io
import time
from pypdf import PdfReader
from docx import Document
import pandas as pd
import subprocess
import os

# --- 1. å®šç¾©å­¸ç§‘èˆ‡é¡Œå‹æ˜ å°„ ---
SUBJECT_Q_TYPES = {
    "åœ‹èª": ["åœ‹å­—æ³¨éŸ³", "é€ å¥", "å–®é¸é¡Œ", "é–±è®€ç´ é¤Šé¡Œ", "å¥å‹è®Šæ›", "ç°¡ç­”é¡Œ"],
    "æ•¸å­¸": ["æ‡‰ç”¨è¨ˆç®—é¡Œ", "åœ–è¡¨åˆ†æé¡Œ", "å¡«å……é¡Œ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ"],
    "è‡ªç„¶ç§‘å­¸": ["å¯¦é©—åˆ¤è®€é¡Œ", "åœ–è¡¨åˆ†æé¡Œ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ", "å¡«å……é¡Œ", "é…åˆé¡Œ"],
    "ç¤¾æœƒ": ["åœ°åœ–åˆ¤è®€é¡Œ", "æƒ…å¢ƒæ¡ˆä¾‹åˆ†æ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ", "é…åˆé¡Œ", "ç°¡ç­”é¡Œ"],
    "è‹±èª": ["è‹±èªæœƒè©±é¸æ“‡", "è©å½™æ­é…", "æ–‡æ„é¸å¡«", "å–®é¸é¡Œ", "é–±è®€ç†è§£"],
    "": ["å–®é¸é¡Œ", "æ˜¯éé¡Œ", "å¡«å……é¡Œ", "ç°¡ç­”é¡Œ"]
}

# --- 2. æª”æ¡ˆè®€å–å·¥å…· (åŠ å…¥å¿«å–å„ªåŒ– ğŸš€) ---
# é€é @st.cache_dataï¼Œè®“ç¨‹å¼è¨˜ä½è®€éçš„æª”æ¡ˆï¼Œä¸ç”¨æ¯æ¬¡éƒ½é‡è®€
@st.cache_data
def extract_text_from_files(files):
    text_content = ""
    for file in files:
        try:
            ext = file.name.split('.')[-1].lower()
            if ext == 'pdf':
                pdf_reader = PdfReader(file)
                text_content += "".join([p.extract_text() or "" for p in pdf_reader.pages])
            elif ext == 'docx':
                doc = Document(file)
                text_content += "\n".join([p.text for p in doc.paragraphs])
            elif ext == 'doc':
                # .doc è™•ç†è¼ƒè¤‡é›œï¼Œé€šå¸¸ä¸å¿«å–æˆ–éœ€ç‰¹æ®Šè™•ç†ï¼Œé€™è£¡ç¶­æŒåŸæ¨£
                with open("temp.doc", "wb") as f: f.write(file.getbuffer())
                result = subprocess.run(['antiword', 'temp.doc'], capture_output=True, text=True)
                if result.returncode == 0:
                    text_content += result.stdout
                if os.path.exists("temp.doc"): os.remove("temp.doc")
        except Exception as e:
            text_content += f"\n[è®€å–éŒ¯èª¤: {file.name}]"
    return text_content

# --- 3. Excel ä¸‹è¼‰å·¥å…· ---
def md_to_excel(md_text):
    try:
        lines = [l for l in md_text.strip().split('\n') if l.startswith('|')]
        if len(lines) < 3: return None
        headers = [c.strip() for c in lines[0].split('|') if c.strip()]
        data = [[c.strip() for c in l.split('|') if c.strip()] for l in lines[2:]]
        df = pd.DataFrame(data, columns=headers)
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False, sheet_name='å­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨')
        return output.getvalue()
    except: return None

# --- 4. æ ¸å¿ƒ Gem å‘½é¡Œéµå¾‹ (å„ªåŒ–æŒ‡ä»¤ï¼šæ¸›å°‘å»¢è©±) ---
GEM_INSTRUCTIONS = """
ä½ æ˜¯ã€Œåœ‹å°å°ˆæ¥­å®šæœŸè©•é‡å‘½é¡Œ AIã€ã€‚
1. **ç§‘ç›®å®ˆé–€å“¡**ï¼šè‹¥æ•™æèˆ‡ç§‘ç›®æ˜é¡¯ä¸ç¬¦ï¼Œåƒ…å›è¦†ï¼šã€ERROR_SUBJECT_MISMATCHã€ã€‚
2. **ç›®æ¨™å°æ‡‰**ï¼šå­¸ç¿’ç›®æ¨™å¿…é ˆåŸæ–‡æ¡è‡ªæ•™æã€‚æ¯ä¸€æ¢ç›®æ¨™åœ¨æ•´ä»½è©¦å·ä¸­è‡³å°‘å‡ºç¾ä¸€æ¬¡ã€‚
3. **ç›´æ¥è¼¸å‡º**ï¼šè«‹ç›´æ¥ç”¢å‡ºè¡¨æ ¼æˆ–è©¦é¡Œï¼Œä¸è¦æœ‰ä»»ä½•é–‹å ´ç™½ï¼ˆå¦‚ã€Œå¥½çš„ï¼Œé€™æ˜¯...ã€ï¼‰æˆ–çµå°¾èªï¼Œä»¥ç¯€çœç”Ÿæˆæ™‚é–“ã€‚
"""

# --- 5. æ™ºèƒ½æ¨¡å‹é¸æ“‡å™¨ ---
def get_best_model(api_key, mode="fast"):
    genai.configure(api_key=api_key)
    try:
        models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        if not models: return None, "æ‰¾ä¸åˆ°å¯ç”¨æ¨¡å‹"
        target_model = None
        if mode == "fast":
            for m in models:
                if 'flash' in m.lower(): target_model = m; break
            if not target_model:
                for m in models:
                    if 'gemini-pro' in m.lower() and 'vision' not in m.lower(): target_model = m; break
        elif mode == "smart":
            for m in models:
                if 'pro' in m.lower() and '1.5' in m.lower(): target_model = m; break
            if not target_model:
                for m in models:
                    if 'pro' in m.lower(): target_model = m; break
        if not target_model: target_model = models[0]
        return target_model, None
    except Exception as e: return None, str(e)

# --- 6. ç¶²é ä»‹é¢è¦–è¦ºè¨­è¨ˆ ---
st.set_page_config(page_title="å…§æ¹–åœ‹å° AI è¼”åŠ©å‡ºé¡Œç³»çµ±", layout="wide")

st.markdown("""
    <style>
    .stApp { background-color: #0F172A; }
    .block-container { max-width: 1200px; padding-top: 1rem; padding-bottom: 5rem; }
    
    .school-header {
        background: linear-gradient(90deg, #1E293B 0%, #334155 100%);
        padding: 25px; border-radius: 18px; text-align: center; margin-bottom: 25px; 
        border: 1px solid #475569;
    }
    .school-name { font-size: 26px; font-weight: 700; color: #F1F5F9; letter-spacing: 3px; }
    .app-title { font-size: 15px; color: #94A3B8; margin-top: 6px; }
    h1, h2, h3, p, span, label, .stMarkdown { color: #E2E8F0 !important; }
    
    /* èˆ’é©å‹å¡ç‰‡ (é–“è·æ‹‰å¯¬å„ªåŒ–) */
    .comfort-box {
        background-color: #1E293B; 
        padding: 15px;               /* å¢åŠ å…§è· */
        border-radius: 10px; 
        margin-bottom: 15px;         /* å¢åŠ å¡ç‰‡é–“è· */
        border-left: 5px solid #3B82F6; 
        font-size: 14px;             /* å­—é«”ç¨å¾®åŠ å¤§ */
        color: #CBD5E1; 
        line-height: 1.8;            /* è¡Œé«˜å¢åŠ ï¼Œé–±è®€ä¸æ“æ“  */
    }
    .comfort-box b { color: #fff; }
    .comfort-box a { color: #60A5FA !important; text-decoration: none; font-weight: bold; }
    .comfort-box a:hover { text-decoration: underline; }
    .comfort-box ul { margin: 0; padding-left: 1.2rem; }
    .comfort-box li { margin-bottom: 5px; } /* åˆ—è¡¨é …ç›®é–“è· */

    /* å´é‚Šæ¬„å…ƒä»¶èˆ’é©åŒ– */
    [data-testid="stSidebar"] .stMarkdown { margin-bottom: 10px; } 
    .stTextArea textarea { min-height: 80px; } /* è¼¸å…¥æ¡†æ‹‰é«˜ */
    .stTextArea { margin-bottom: 15px !important; }
    [data-testid="stSidebar"] .stButton > button { 
        display: block; margin: 15px auto !important; /* æŒ‰éˆ•ä¸Šä¸‹ç•™ç™½ */
        width: 100%; border-radius: 8px; height: 42px; /* æŒ‰éˆ•åŠ å¤§ */
        background-color: #334155; border: 1px solid #475569;
        font-size: 15px;
    }
    
    .footer { position: fixed; left: 0; bottom: 0; width: 100%; background-color: #0F172A; color: #475569; text-align: center; padding: 12px; font-size: 11px; border-top: 1px solid #1E293B; z-index: 100; }
    </style>
    
    <div class="school-header">
        <div class="school-name">æ–°ç«¹å¸‚é¦™å±±å€å…§æ¹–åœ‹å°</div>
        <div class="app-title">è©•é‡å‘½é¡Œèˆ‡å­¸ç¿’ç›®æ¨™è‡ªå‹•åŒ–ç³»çµ±</div>
    </div>
    """, unsafe_allow_html=True)

# ç‹€æ…‹ç®¡ç†
if "phase" not in st.session_state: st.session_state.phase = 1 
if "chat_history" not in st.session_state: st.session_state.chat_history = []
if "last_prompt_content" not in st.session_state: st.session_state.last_prompt_content = ""

# --- Sidebar: èˆ’é©ç‰ˆ (é–“è·æ‹‰å¯¬) ---
with st.sidebar:
    st.markdown("### ğŸš€ å¿«é€ŸæŒ‡å—")
    
    st.markdown("""
    <div class="comfort-box">
        <ol style="margin:0; padding-left:1.2rem;">
            <li>å‰å¾€ <a href="https://aistudio.google.com/" target="_blank">Google AI Studio (é»æˆ‘)</a></li>
            <li>ç™»å…¥<b>å€‹äºº Google å¸³è™Ÿ</b> (é¿é–‹æ•™è‚²ç‰ˆ)</li>
            <li>é»æ“Š <b>Get API key</b> ä¸¦è¤‡è£½</li>
            <li>è²¼å…¥ä¸‹æ–¹æ¬„ä½</li>
        </ol>
    </div>
    """, unsafe_allow_html=True)
    
    api_input = st.text_area("åœ¨æ­¤è¼¸å…¥ API Key", height=80, placeholder="è«‹è²¼ä¸Šé‡‘é‘°...")
    
    if st.button("ğŸ”„ é‡ç½®ç³»çµ±"):
        st.session_state.phase = 1
        st.session_state.chat_history = []
        st.session_state.last_prompt_content = ""
        st.rerun()

    st.markdown("### ğŸ“š è³‡æºé€£çµ")
    st.markdown("""
    <div class="comfort-box">
        <b>æ•™æä¸‹è¼‰ï¼š</b><br>
        â€¢ <a href="https://webetextbook.knsh.com.tw/" target="_blank">åº·è»’é›»å­æ›¸</a><br>
        â€¢ <a href="https://edisc3.hle.com.tw/" target="_blank">ç¿°æ—è¡Œå‹•å¤§å¸«</a><br>
        â€¢ <a href="https://reader.nani.com.tw/" target="_blank">å—ä¸€ OneBox</a><br>
        <br>
        <b>åƒè€ƒè³‡æ–™ï¼š</b><br>
        â€¢ <a href="https://cirn.moe.edu.tw/Syllabus/index.aspx?sid=1108" target="_blank">108èª²ç¶±è³‡æºç¶² (CIRN)</a><br>
        â€¢ <a href="https://www.nhps.hc.edu.tw/" target="_blank">å…§æ¹–åœ‹å°æ ¡ç¶²</a>
    </div>
    """, unsafe_allow_html=True)

# --- Phase 1: åƒæ•¸è¨­å®šèˆ‡æ•™æä¸Šå‚³ ---
if st.session_state.phase == 1:
    with st.container(border=True):
        st.markdown("### ğŸ“ ç¬¬ä¸€éšæ®µï¼šåƒæ•¸è¨­å®šèˆ‡æ•™æä¸Šå‚³")
        
        c1, c2, c3 = st.columns(3)
        with c1: grade = st.selectbox("1. é¸æ“‡å¹´ç´š", ["", "ä¸€å¹´ç´š", "äºŒå¹´ç´š", "ä¸‰å¹´ç´š", "å››å¹´ç´š", "äº”å¹´ç´š", "å…­å¹´ç´š"], index=0)
        with c2: subject = st.selectbox("2. é¸æ“‡ç§‘ç›®", ["", "åœ‹èª", "æ•¸å­¸", "è‡ªç„¶ç§‘å­¸", "ç¤¾æœƒ", "è‹±èª"], index=0)
        with c3: mode = st.selectbox("3. å‘½é¡Œæ¨¡å¼", ["ğŸŸ¢ æ¨¡å¼ Aï¼šé©ä¸­", "ğŸ”´ æ¨¡å¼ Bï¼šå›°é›£", "ğŸŒŸ æ¨¡å¼ Cï¼šç´ é¤Š"], index=0)
        
        st.divider()
        st.markdown("**4. å‹¾é¸æ¬²ç”¢å‡ºçš„é¡Œå‹**")
        available_types = SUBJECT_Q_TYPES.get(subject, SUBJECT_Q_TYPES[""])
        cols = st.columns(min(len(available_types), 4))
        selected_types = []
        for i, t in enumerate(available_types):
            if cols[i % len(cols)].checkbox(t, value=True):
                selected_types.append(t)
        
        st.divider()
        uploaded_files = st.file_uploader("5. ä¸Šå‚³æ•™ææª”æ¡ˆ (Word/PDF)", type=["pdf", "docx", "doc"], accept_multiple_files=True)
        
        if st.button("ğŸš€ ç”¢å‡ºå­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨", type="primary", use_container_width=True):
            if not api_input:
                st.error("âŒ å‹•ä½œä¸­æ­¢ï¼šå´é‚Šæ¬„å°šæœªè¼¸å…¥ API Keyã€‚")
            elif not grade or not subject or not uploaded_files or not selected_types:
                st.warning("âš ï¸ å‹•ä½œä¸­æ­¢ï¼šè«‹ç¢ºèªå¹´ç´šã€ç§‘ç›®ã€é¡Œå‹èˆ‡æ•™æå·²å‚™å¦¥ã€‚")
            else:
                with st.spinner("âš¡ æ­£åœ¨æ¥µé€Ÿæƒææ•™æå…§å®¹ï¼Œè«‹ç¨å€™..."):
                    keys = [k.strip() for k in api_input.replace('\n', ',').split(',') if k.strip()]
                    target_key = random.choice(keys)
                    model_name, error_msg = get_best_model(target_key, mode="fast")
                    
                    if error_msg:
                        st.error(f"âŒ API é€£ç·šéŒ¯èª¤ï¼š{error_msg}")
                    else:
                        # ä½¿ç”¨å¿«å–å‡½æ•¸è®€å–æª”æ¡ˆ (æ•ˆèƒ½å„ªåŒ–é—œéµ ğŸš€)
                        content = extract_text_from_files(uploaded_files)
                        
                        try:
                            st.toast(f"âš¡ å•Ÿå‹• AI å¼•æ“ ({model_name}) åˆ†æä¸­...", icon="ğŸ¤–")
                            
                            model_fast = genai.GenerativeModel(
                                model_name=model_name,
                                system_instruction=GEM_INSTRUCTIONS, 
                                generation_config={"temperature": 0.0}
                            )
                            
                            chat = model_fast.start_chat(history=[])
                            
                            with st.chat_message("ai"):
                                message_placeholder = st.empty()
                                full_response = ""
                                t_str = "ã€".join(selected_types)
                                prompt_content = f"å¹´ç´šï¼š{grade}, ç§‘ç›®ï¼š{subject}\né¡Œå‹ï¼š{t_str}\næ•™æå…§å®¹ï¼š\n{content}"
                                st.session_state.last_prompt_content = prompt_content
                                
                                response = chat.send_message(prompt_content, stream=True)
                                
                                for chunk in response:
                                    full_response += chunk.text
                                    message_placeholder.markdown(full_response + "â–Œ")
                                message_placeholder.markdown(full_response)
                            
                            if "ERROR_SUBJECT_MISMATCH" in full_response:
                                st.error(f"âŒ é˜²å‘†å•Ÿå‹•ï¼šæ•™æå…§å®¹èˆ‡ã€{subject}ã€ä¸ç¬¦ï¼Œè«‹é‡æ–°ç¢ºèªæª”æ¡ˆã€‚")
                            else:
                                st.session_state.chat_history.append({"role": "model", "content": full_response})
                                st.session_state.phase = 2
                                st.rerun()
                        except Exception as e: st.error(f"é€£ç·šå¤±æ•—ï¼š{e}")

# --- Phase 2: æ­£å¼å‡ºé¡Œ ---
elif st.session_state.phase == 2:
    current_md = st.session_state.chat_history[0]["content"]
    
    with st.container(border=True):
        st.markdown("### ğŸ“¥ ç¬¬äºŒéšæ®µï¼šä¸‹è¼‰å¯©æ ¸è¡¨")
        with st.chat_message("ai"): st.markdown(current_md)
        excel_data = md_to_excel(current_md)
        if excel_data:
            st.download_button(label="ğŸ“¥ åŒ¯å‡ºæ­¤å¯©æ ¸è¡¨ (Excel)", data=excel_data, file_name=f"å…§æ¹–åœ‹å°_{subject}_å¯©æ ¸è¡¨.xlsx", use_container_width=True)

    st.divider()
    with st.container(border=True):
        st.markdown("### ğŸ“ ç¬¬ä¸‰éšæ®µï¼šè©¦å·æ­£å¼ç”Ÿæˆ")
        
        cb1, cb2 = st.columns(2)
        with cb1:
            if st.button("âœ… å¯©æ ¸è¡¨ç¢ºèªç„¡èª¤ï¼Œé–‹å§‹å‡ºé¡Œ", type="primary", use_container_width=True):
                with st.spinner("ğŸ§  æ­£åœ¨é€²è¡Œæ·±åº¦æ¨ç†å‘½é¡Œï¼Œè«‹ç¨å€™..."):
                    keys = [k.strip() for k in api_input.replace('\n', ',').split(',') if k.strip()]
                    target_key = random.choice(keys)
                    model_name, error_msg = get_best_model(target_key, mode="smart")
                    
                    if error_msg:
                         st.error(f"âŒ ç„¡æ³•å•Ÿå‹•é«˜éšæ¨¡å‹ï¼š{error_msg}")
                    else:
                        st.toast(f"ğŸ§  åˆ‡æ›è‡³æ·±åº¦æ€è€ƒæ¨¡å¼ ({model_name})...", icon="ğŸ’¡")
                        
                        try:
                            model_smart = genai.GenerativeModel(
                                model_name=model_name,
                                system_instruction=GEM_INSTRUCTIONS,
                                generation_config={"temperature": 0.2}
                            )
                            
                            with st.chat_message("ai"):
                                message_placeholder = st.empty()
                                full_response = ""
                                final_prompt = f"""
                                {st.session_state.last_prompt_content}
                                ---
                                å¯©æ ¸è¡¨åƒè€ƒï¼š
                                {current_md}
                                
                                è«‹æ­£å¼ç”¢å‡ºã€è©¦é¡Œã€‘èˆ‡ã€åƒè€ƒç­”æ¡ˆå·ã€‘ã€‚
                                """
                                response = model_smart.generate_content(final_prompt, stream=True)
                                for chunk in response:
                                    full_response += chunk.text
                                    message_placeholder.markdown(full_response + "â–Œ")
                                message_placeholder.markdown(full_response)
                            
                            st.session_state.chat_history.append({"role": "model", "content": full_response})
                        except Exception as e: st.error(f"å‘½é¡Œå¤±æ•—ï¼š{e}")

        with cb2:
            if st.button("â¬…ï¸ è¿”å›ä¿®æ”¹åƒæ•¸", use_container_width=True):
                st.session_state.phase = 1
                st.session_state.chat_history = []
                st.rerun()
    
    # å¾®èª¿å°è©±æ¡†
    if len(st.session_state.chat_history) > 1:
        if prompt := st.chat_input("å°é¡Œç›®ä¸æ»¿æ„ï¼Ÿè«‹è¼¸å…¥æŒ‡ä»¤å¾®èª¿"):
            with st.chat_message("user"): st.markdown(prompt)
            
            with st.spinner("ğŸ”§ AI æ­£åœ¨ä¿®æ”¹è©¦é¡Œ..."):
                keys = [k.strip() for k in api_input.replace('\n', ',').split(',') if k.strip()]
                genai.configure(api_key=random.choice(keys))
                model_pro = genai.GenerativeModel("gemini-1.5-pro", system_instruction=GEM_INSTRUCTIONS)
                
                history_for_chat = []
                history_for_chat.append({"role": "user", "parts": [st.session_state.last_prompt_content]})
                history_for_chat.append({"role": "model", "parts": [current_md]})
                if len(st.session_state.chat_history) > 1:
                     history_for_chat.append({"role": "model", "parts": [st.session_state.chat_history[-1]["content"]]})
                
                chat_pro = model_pro.start_chat(history=history_for_chat)
                
                with st.chat_message("ai"):
                    message_placeholder = st.empty()
                    full_response = ""
                    response = chat_pro.send_message(prompt, stream=True)
                    for chunk in response:
                        full_response += chunk.text
                        message_placeholder.markdown(full_response + "â–Œ")
                    message_placeholder.markdown(full_response)
                
                st.session_state.chat_history.append({"role": "model", "content": full_response})

st.markdown('<div class="footer">Â© 2026 æ–°ç«¹å¸‚é¦™å±±å€å…§æ¹–åœ‹å°. All Rights Reserved.</div>', unsafe_allow_html=True)
