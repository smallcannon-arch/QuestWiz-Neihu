import streamlit as st
import google.generativeai as genai
import random
import io
import time
from pypdf import PdfReader
from docx import Document
import pandas as pd
import subprocess
import os
import re # æ–°å¢ï¼šç”¨æ–¼æ–‡å­—æ¸…æ´—

# --- 1. å®šç¾©å­¸ç§‘èˆ‡é¡Œå‹æ˜ å°„ ---
SUBJECT_Q_TYPES = {
    "åœ‹èª": ["åœ‹å­—æ³¨éŸ³", "é€ å¥", "å–®é¸é¡Œ", "é–±è®€ç´ é¤Šé¡Œ", "å¥å‹è®Šæ›", "ç°¡ç­”é¡Œ"],
    "æ•¸å­¸": ["æ‡‰ç”¨è¨ˆç®—é¡Œ", "åœ–è¡¨åˆ†æé¡Œ", "å¡«å……é¡Œ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ"],
    "è‡ªç„¶ç§‘å­¸": ["å¯¦é©—åˆ¤è®€é¡Œ", "åœ–è¡¨åˆ†æé¡Œ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ", "å¡«å……é¡Œ", "é…åˆé¡Œ"],
    "ç¤¾æœƒ": ["åœ°åœ–åˆ¤è®€é¡Œ", "æƒ…å¢ƒæ¡ˆä¾‹åˆ†æ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ", "é…åˆé¡Œ", "ç°¡ç­”é¡Œ"],
    "è‹±èª": ["è‹±èªæœƒè©±é¸æ“‡", "è©å½™æ­é…", "æ–‡æ„é¸å¡«", "å–®é¸é¡Œ", "é–±è®€ç†è§£"],
    "": ["å–®é¸é¡Œ", "æ˜¯éé¡Œ", "å¡«å……é¡Œ", "ç°¡ç­”é¡Œ"]
}

# --- 2. æª”æ¡ˆè®€å–å·¥å…· (å¼·åŒ–ç‰ˆï¼šåˆ†é +æ¸…æ´—) ---
@st.cache_data
def extract_text_from_files(files):
    text_content = ""
    for file in files:
        try:
            file_text = ""
            ext = file.name.split('.')[-1].lower()
            
            if ext == 'pdf':
                pdf_reader = PdfReader(file)
                # åŠ ä¸Šé ç¢¼æ¨™è¨˜ï¼Œå¹«åŠ© AI å€åˆ†å–®å…ƒé‚Šç•Œ
                for i, page in enumerate(pdf_reader.pages):
                    content = page.extract_text() or ""
                    file_text += f"\n--- Page {i+1} ---\n{content}"
            elif ext == 'docx':
                doc = Document(file)
                # ä¿ç•™æ®µè½çµæ§‹
                file_text = "\n".join([p.text for p in doc.paragraphs])
            elif ext == 'doc':
                with open("temp.doc", "wb") as f: f.write(file.getbuffer())
                result = subprocess.run(['antiword', 'temp.doc'], capture_output=True, text=True)
                if result.returncode == 0:
                    file_text = result.stdout
                if os.path.exists("temp.doc"): os.remove("temp.doc")
            
            # --- æ–‡å­—æ¸…æ´—å€ ---
            # 1. ç§»é™¤é€£çºŒå¤šé¤˜çš„ç©ºè¡Œï¼Œç¸®æ¸› Token
            file_text = re.sub(r'\n\s*\n', '\n\n', file_text)
            text_content += f"\n\n=== æª”æ¡ˆ: {file.name} ===\n{file_text}"
            
        except Exception as e:
            text_content += f"\n[è®€å–éŒ¯èª¤: {file.name} - {str(e)}]"
            
    return text_content

# --- 3. æ ¸å¿ƒ Gem å‘½é¡Œéµå¾‹ (Phase 1 å°ˆç”¨ï¼šå¯©æ ¸è¡¨ç”Ÿæˆ) ---
# é€™è£¡ç¨å¾®ä¿®æ”¹ï¼Œæ•™å° AI å¦‚ä½•ã€Œåˆ†é…åˆ†æ•¸ã€
GEM_INSTRUCTIONS_PHASE1 = """
ä½ æ˜¯ã€Œåœ‹å°å°ˆæ¥­å®šæœŸè©•é‡å‘½é¡Œ AIã€ã€‚

### âš ï¸ Phase 1 ä»»å‹™ç›®æ¨™ï¼š
è«‹é–±è®€ä½¿ç”¨è€…æä¾›çš„æ•™æå…§å®¹ï¼Œæ•´ç†å‡ºä¸€ä»½ã€å­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨ã€‘ã€‚

### çµ•å°è¦å‰‡ (é•åå°‡å°è‡´ç³»çµ±å´©æ½°)ï¼š
1. **é…åˆ†é‚è¼¯**ï¼šè«‹æ ¹æ“šå„å–®å…ƒå…§å®¹çš„ã€Œç¯‡å¹…é•·åº¦ã€èˆ‡ã€Œé‡è¦æ€§ã€ï¼Œå°‡ç¸½åˆ†åˆ†é…ç‚º **å‰›å¥½ 100 åˆ†**ã€‚
2. **ç¦æ­¢å»¢è©±**ï¼š**åš´ç¦** æ’°å¯«å‰è¨€ (å¦‚ "å¥½çš„ï¼Œé€™æ˜¯æˆ‘æ•´ç†çš„...") æˆ–çµèªã€‚
3. **ç¦æ­¢å‡ºé¡Œ**ï¼šç¾åœ¨é‚„ä¸æ˜¯å‡ºé¡Œéšæ®µï¼Œ**åš´ç¦** ç”¢å‡ºé¡Œç›®ã€‚
4. **æ ¼å¼è¦æ±‚**ï¼š
   - åƒ…è¼¸å‡ºæ¨™æº– Markdown è¡¨æ ¼ã€‚
   - æ¬„ä½å¿…é ˆåŒ…å«ï¼š| å–®å…ƒåç¨± | å­¸ç¿’ç›®æ¨™(åŸæ–‡) | å°æ‡‰é¡Œå‹ | é è¨ˆé…åˆ† |
   - **æ¯ä¸€åˆ—è³‡æ–™å¿…é ˆå¼·åˆ¶æ›è¡Œ**ï¼Œä¸å¯æ¥åœ¨åŒä¸€è¡Œã€‚
"""

# --- 4. æ™ºèƒ½æ¨¡å‹é¸æ“‡èˆ‡é‡è©¦æ©Ÿåˆ¶ ---
def get_best_model(api_key, mode="fast"):
    genai.configure(api_key=api_key)
    try:
        models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        if not models: return None, "æ‰¾ä¸åˆ°å¯ç”¨æ¨¡å‹"
        target_model = None
        # å„ªå…ˆé¸æ“‡ flash æ¨¡å‹ä»¥æ±‚å¿«é€Ÿèˆ‡é•·æ–‡æœ¬è™•ç†èƒ½åŠ›
        if mode == "fast":
            for m in models:
                if 'flash' in m.lower(): target_model = m; break
            if not target_model: target_model = models[0]
        # é€™è£¡ä¿ç•™ smart é‚è¼¯çµ¦ç¬¬äºŒéšæ®µç”¨
        elif mode == "smart":
            for m in models:
                if 'pro' in m.lower() and '1.5' in m.lower(): target_model = m; break
            if not target_model: target_model = models[0]
            
        return target_model, None
    except Exception as e: return None, str(e)

def generate_with_retry(model_or_chat, prompt, stream=True):
    max_retries = 3
    for i in range(max_retries):
        try:
            if hasattr(model_or_chat, 'send_message'):
                return model_or_chat.send_message(prompt, stream=stream)
            else:
                return model_or_chat.generate_content(prompt, stream=stream)
        except Exception as e:
            if "429" in str(e):
                wait_time = (i + 1) * 3
                st.toast(f"â³ ä¼ºæœå™¨å¿™ç¢Œï¼Œä¼‘æ¯ {wait_time} ç§’å¾Œå†è©¦...", icon="â˜•")
                time.sleep(wait_time)
            else:
                raise e
    raise Exception("é€£ç·šé€¾æ™‚ï¼Œè«‹æª¢æŸ¥ API Key æˆ–ç¶²è·¯ç‹€æ…‹ã€‚")

# --- 5. ç¶²é ä»‹é¢é…ç½® ---
st.set_page_config(page_title="å…§æ¹–åœ‹å° AI è¼”åŠ©å‡ºé¡Œç³»çµ±", layout="wide")

# (ä¿ç•™åŸæœ¬çš„ CSS æ¨£å¼)
st.markdown("""
    <style>
    header[data-testid="stHeader"] { display: none !important; visibility: hidden !important; }
    footer { display: none !important; visibility: hidden !important; }
    .stApp { background-color: #0F172A; }
    .block-container { max-width: 1200px; padding-top: 1.5rem !important; padding-bottom: 5rem; }
    .school-header {
        background: linear-gradient(90deg, #1E293B 0%, #334155 100%);
        padding: 25px; border-radius: 18px; text-align: center; margin-bottom: 25px; 
        border: 1px solid #475569;
    }
    .school-name { font-size: 26px; font-weight: 700; color: #F1F5F9; letter-spacing: 3px; }
    .app-title { font-size: 15px; color: #94A3B8; margin-top: 6px; }
    h1, h2, h3, p, span, label, .stMarkdown { color: #E2E8F0 !important; }
    .comfort-box {
        background-color: #1E293B; padding: 15px; border-radius: 10px; 
        margin-bottom: 15px; border-left: 5px solid #3B82F6; 
        font-size: 14px; color: #CBD5E1; line-height: 1.8;
    }
    .comfort-box b { color: #fff; }
    .comfort-box a { color: #60A5FA !important; text-decoration: none; font-weight: bold; }
    [data-testid="stSidebar"] .stMarkdown { margin-bottom: 10px; } 
    .stTextArea textarea { min-height: 80px; }
    .stTextArea { margin-bottom: 15px !important; }
    [data-testid="stSidebar"] .stButton > button { 
        display: block; margin: 15px auto !important; 
        width: 100%; border-radius: 8px; height: 42px;
        background-color: #334155; border: 1px solid #475569; font-size: 15px;
    }
    .custom-footer { 
        position: fixed; left: 0; bottom: 0; width: 100%; 
        background-color: #0F172A; color: #475569; 
        text-align: center; padding: 12px; font-size: 11px; 
        border-top: 1px solid #1E293B; z-index: 100; 
    }
    </style>
    <div class="school-header">
        <div class="school-name">æ–°ç«¹å¸‚é¦™å±±å€å…§æ¹–åœ‹å°</div>
        <div class="app-title">è©•é‡å‘½é¡Œèˆ‡å­¸ç¿’ç›®æ¨™è‡ªå‹•åŒ–ç³»çµ±</div>
    </div>
    """, unsafe_allow_html=True)

# ç‹€æ…‹ç®¡ç†
if "phase" not in st.session_state: st.session_state.phase = 1 
if "chat_history" not in st.session_state: st.session_state.chat_history = []
if "last_prompt_content" not in st.session_state: st.session_state.last_prompt_content = ""

# --- Sidebar ---
with st.sidebar:
    st.markdown("### ğŸš€ ç³»çµ±è¨­å®š")
    api_input = st.text_area("åœ¨æ­¤è¼¸å…¥ API Key", height=80, placeholder="è«‹è²¼ä¸Š Google AI Studio é‡‘é‘°...")
    
    if st.button("ğŸ”„ é‡ç½®ç³»çµ±"):
        st.session_state.phase = 1
        st.session_state.chat_history = []
        st.session_state.last_prompt_content = ""
        st.rerun()

    st.markdown("### ğŸ“š è³‡æºé€£çµ")
    st.markdown("""
    <div class="comfort-box">
        <b>æ•™æä¸‹è¼‰ï¼š</b><br>
        â€¢ <a href="https://webetextbook.knsh.com.tw/" target="_blank">åº·è»’é›»å­æ›¸</a><br>
        â€¢ <a href="https://edisc3.hle.com.tw/" target="_blank">ç¿°æ—è¡Œå‹•å¤§å¸«</a><br>
        â€¢ <a href="https://reader.nani.com.tw/" target="_blank">å—ä¸€ OneBox</a>
    </div>
    """, unsafe_allow_html=True)

# --- Phase 1: åƒæ•¸è¨­å®šèˆ‡æ•™æä¸Šå‚³ ---
if st.session_state.phase == 1:
    with st.container(border=True):
        st.markdown("### ğŸ“ ç¬¬ä¸€éšæ®µï¼šåƒæ•¸è¨­å®šèˆ‡æ•™æä¸Šå‚³")
        
        c1, c2, c3 = st.columns(3)
        with c1: grade = st.selectbox("1. é¸æ“‡å¹´ç´š", ["", "ä¸€å¹´ç´š", "äºŒå¹´ç´š", "ä¸‰å¹´ç´š", "å››å¹´ç´š", "äº”å¹´ç´š", "å…­å¹´ç´š"], index=0)
        with c2: subject = st.selectbox("2. é¸æ“‡ç§‘ç›®", ["", "åœ‹èª", "æ•¸å­¸", "è‡ªç„¶ç§‘å­¸", "ç¤¾æœƒ", "è‹±èª"], index=0)
        with c3: mode = st.selectbox("3. å‘½é¡Œæ¨¡å¼", ["ğŸŸ¢ æ¨¡å¼ Aï¼šé©ä¸­", "ğŸ”´ æ¨¡å¼ Bï¼šå›°é›£", "ğŸŒŸ æ¨¡å¼ Cï¼šç´ é¤Š"], index=0)
        
        st.divider()
        st.markdown("**4. å‹¾é¸æ¬²ç”¢å‡ºçš„é¡Œå‹**")
        available_types = SUBJECT_Q_TYPES.get(subject, SUBJECT_Q_TYPES[""])
        cols = st.columns(min(len(available_types), 4))
        selected_types = []
        for i, t in enumerate(available_types):
            if cols[i % len(cols)].checkbox(t, value=True):
                selected_types.append(t)
        
        st.divider()
        uploaded_files = st.file_uploader("5. ä¸Šå‚³æ•™ææª”æ¡ˆ (Word/PDF)", type=["pdf", "docx", "doc"], accept_multiple_files=True)
        
        if st.button("ğŸš€ ç”¢å‡ºå­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨", type="primary", use_container_width=True):
            if not api_input:
                st.error("âŒ å‹•ä½œä¸­æ­¢ï¼šå´é‚Šæ¬„å°šæœªè¼¸å…¥ API Keyã€‚")
            elif not grade or not subject or not uploaded_files or not selected_types:
                st.warning("âš ï¸ å‹•ä½œä¸­æ­¢ï¼šè«‹ç¢ºèªå¹´ç´šã€ç§‘ç›®ã€é¡Œå‹èˆ‡æ•™æå·²å‚™å¦¥ã€‚")
            else:
                with st.spinner("âš¡ æ­£åœ¨æ¥µé€Ÿæƒææ•™æå…§å®¹ï¼Œè«‹ç¨å€™..."):
                    # 1. æº–å‚™ API
                    keys = [k.strip() for k in api_input.replace('\n', ',').split(',') if k.strip()]
                    target_key = random.choice(keys)
                    model_name, error_msg = get_best_model(target_key, mode="fast")
                    
                    if error_msg:
                        st.error(f"âŒ API é€£ç·šéŒ¯èª¤ï¼š{error_msg}")
                    else:
                        # 2. è®€å–ä¸¦æ¸…æ´—æª”æ¡ˆ
                        content = extract_text_from_files(uploaded_files)
                        
                        try:
                            st.toast(f"âš¡ å•Ÿå‹• AI å¼•æ“ ({model_name}) åˆ†æä¸­...", icon="ğŸ¤–")
                            
                            # 3. è¨­å®š Phase 1 å°ˆç”¨æ¨¡å‹
                            model_fast = genai.GenerativeModel(
                                model_name=model_name,
                                system_instruction=GEM_INSTRUCTIONS_PHASE1, 
                                generation_config={"temperature": 0.0} # æº«åº¦ 0 ç¢ºä¿æ ¼å¼æœ€ç©©å®š
                            )
                            
                            chat = model_fast.start_chat(history=[])
                            
                            with st.chat_message("ai"):
                                message_placeholder = st.empty()
                                full_response = ""
                                t_str = "ã€".join(selected_types)
                                
                                # 4. æ§‹å»ºç²¾æº– Prompt
                                prompt_content = f"""
                                ä»»å‹™ï¼šåˆ†æä»¥ä¸‹æ•™æä¸¦ç”¢å‡ºå¯©æ ¸è¡¨ã€‚
                                
                                ã€åƒæ•¸è¨­å®šã€‘
                                å¹´ç´šï¼š{grade}
                                ç§‘ç›®ï¼š{subject}
                                å¯ç”¨é¡Œå‹ï¼š{t_str}
                                
                                ã€æ•™æå…§å®¹ã€‘
                                {content}
                                
                                ã€åŸ·è¡Œæ­¥é©Ÿã€‘
                                1. è­˜åˆ¥æ•™æä¸­çš„å–®å…ƒçµæ§‹ã€‚
                                2. æå–å…·é«”çš„å­¸ç¿’ç›®æ¨™ï¼ˆKey Learning Pointsï¼‰ã€‚
                                3. æ ¹æ“šå…§å®¹é•·åº¦ï¼Œè¨ˆç®—è©²å–®å…ƒæ‡‰ä½”ç¸½åˆ† 100 åˆ†ä¸­çš„å¤šå°‘æ¯”ä¾‹ã€‚
                                4. åƒ…è¼¸å‡º Markdown è¡¨æ ¼ã€‚
                                """
                                st.session_state.last_prompt_content = prompt_content
                                
                                # 5. ä¸²æµè¼¸å‡º
                                response = generate_with_retry(chat, prompt_content, stream=True)
                                
                                for chunk in response:
                                    if chunk.text:
                                        full_response += chunk.text
                                        message_placeholder.markdown(full_response + "â–Œ")
                                message_placeholder.markdown(full_response)
                            
                            # 6. ç‹€æ…‹ä¿å­˜èˆ‡æ›é 
                            # ç°¡å–®é˜²å‘†ï¼šç¢ºä¿æœ‰ç”¢å‡ºè¡¨æ ¼
                            if "|" in full_response and "å–®å…ƒ" in full_response:
                                st.session_state.chat_history.append({"role": "model", "content": full_response})
                                st.session_state.phase = 2
                                time.sleep(1) # ç¨å¾®ç·©è¡è®“ä½¿ç”¨è€…çœ‹æ¸…çµæœ
                                st.rerun()
                            else:
                                st.error("âŒ AI ç”¢å‡ºæ ¼å¼ç•°å¸¸ï¼Œæœªåµæ¸¬åˆ°è¡¨æ ¼ï¼Œè«‹æª¢æŸ¥æ•™ææª”æ¡ˆæ˜¯å¦æ¸…æ™°ã€‚")
                                
                        except Exception as e: 
                            st.error(f"é€£ç·šå¤±æ•—ï¼š{e} (è«‹æª¢æŸ¥ API Key æˆ–ç¨å¾Œé‡è©¦)")

# --- Phase 2: é€™è£¡å…ˆç•™ç™½æˆ–é¡¯ç¤ºç°¡å–®è¨Šæ¯ï¼Œç­‰å¾…ä½ ä¸‹ä¸€æ­¥æŒ‡ä»¤ ---
elif st.session_state.phase == 2:
    st.info("âœ… ç¬¬ä¸€éšæ®µå®Œæˆï¼å¯©æ ¸è¡¨å·²ç”Ÿæˆ (Phase 2 å¾…çºŒ...)")
    current_md = st.session_state.chat_history[0]["content"]
    st.markdown(current_md)
    
    if st.button("â¬…ï¸ è¿”å›é‡ä¾†"):
        st.session_state.phase = 1
        st.session_state.chat_history = []
        st.rerun()

st.markdown('<div class="custom-footer">Â© 2026 æ–°ç«¹å¸‚é¦™å±±å€å…§æ¹–åœ‹å°. All Rights Reserved.</div>', unsafe_allow_html=True)
