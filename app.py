import streamlit as st
import google.generativeai as genai
import random
import io
import time
import re
from pypdf import PdfReader
from docx import Document
import pandas as pd
import subprocess
import os

# --- 1. å®šç¾©å­¸ç§‘èˆ‡é¡Œå‹æ˜ å°„ ---
SUBJECT_Q_TYPES = {
    "åœ‹èª": ["åœ‹å­—æ³¨éŸ³", "é€ å¥", "å–®é¸é¡Œ", "é–±è®€ç´ é¤Šé¡Œ", "å¥å‹è®Šæ›", "ç°¡ç­”é¡Œ"],
    "æ•¸å­¸": ["æ‡‰ç”¨è¨ˆç®—é¡Œ", "åœ–è¡¨åˆ†æé¡Œ", "å¡«å……é¡Œ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ"],
    "è‡ªç„¶ç§‘å­¸": ["å¯¦é©—åˆ¤è®€é¡Œ", "åœ–è¡¨åˆ†æé¡Œ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ", "å¡«å……é¡Œ", "é…åˆé¡Œ"],
    "ç¤¾æœƒ": ["åœ°åœ–åˆ¤è®€é¡Œ", "æƒ…å¢ƒæ¡ˆä¾‹åˆ†æ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ", "é…åˆé¡Œ", "ç°¡ç­”é¡Œ"],
    "è‹±èª": ["è‹±èªæœƒè©±é¸æ“‡", "è©å½™æ­é…", "æ–‡æ„é¸å¡«", "å–®é¸é¡Œ", "é–±è®€ç†è§£"],
    "": ["å–®é¸é¡Œ", "æ˜¯éé¡Œ", "å¡«å……é¡Œ", "ç°¡ç­”é¡Œ"]
}

# --- 2. æª”æ¡ˆè®€å–å·¥å…· ---
@st.cache_data
def extract_text_from_files(files):
    text_content = ""
    for file in files:
        try:
            ext = file.name.split('.')[-1].lower()
            if ext == 'pdf':
                pdf_reader = PdfReader(file)
                text_content += "".join([p.extract_text() or "" for p in pdf_reader.pages])
            elif ext == 'docx':
                doc = Document(file)
                text_content += "\n".join([p.text for p in doc.paragraphs])
            elif ext == 'doc':
                with open("temp.doc", "wb") as f: f.write(file.getbuffer())
                result = subprocess.run(['antiword', 'temp.doc'], capture_output=True, text=True)
                if result.returncode == 0: text_content += result.stdout
                if os.path.exists("temp.doc"): os.remove("temp.doc")
        except: text_content += f"\n[è®€å–éŒ¯èª¤: {file.name}]"
    return text_content

def process_table_data(md_text):
    try:
        cleaned = md_text.replace("ï½œ", "|").replace("**", "").replace("||", "|\n|")
        header_match = re.search(r'\|\s*å–®å…ƒåç¨±\s*\|\s*å­¸ç¿’ç›®æ¨™.*\|\s*å°æ‡‰é¡Œå‹\s*\|\s*é è¨ˆé…åˆ†\s*\|', cleaned)
        if not header_match: return None
        raw_cells = [c.strip() for c in cleaned[header_match.start():].split('|') if c.strip() and '---' not in c]
        num_cols = 4 
        if len(raw_cells) < num_cols: return None
        headers = raw_cells[:num_cols]
        data_cells = raw_cells[num_cols:]
        rows = [data_cells[i:i+num_cols] for i in range(0, len(data_cells), num_cols)]
        for r in rows:
            if len(r) < num_cols: r += [''] * (num_cols - len(r))
        return pd.DataFrame(rows, columns=headers)
    except: return None

# --- 3. æ™ºèƒ½æ¨¡å‹å°‹æ‰¾å™¨ ---
def find_available_model(api_key, keyword="flash"):
    """è‡ªå‹•åµæ¸¬ API Key ä¸‹å¯ç”¨çš„æ¨¡å‹åç¨±"""
    genai.configure(api_key=api_key)
    try:
        # åˆ—å‡ºæ‰€æœ‰æ”¯æ´å…§å®¹ç”Ÿæˆçš„æ¨¡å‹
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        # å„ªå…ˆæ‰¾åŒ…å«é—œéµå­— (å¦‚ flash æˆ– pro) çš„æ¨¡å‹
        target = next((m for m in available_models if keyword in m.lower()), available_models[0])
        return target, None
    except Exception as e:
        return None, str(e)

def generate_with_retry(model_or_chat, prompt, stream=True):
    max_retries = 3
    for i in range(max_retries):
        try:
            if hasattr(model_or_chat, 'send_message'): return model_or_chat.send_message(prompt, stream=stream)
            else: return model_or_chat.generate_content(prompt, stream=stream)
        except Exception as e:
            if "429" in str(e):
                wait = (i + 1) * 5
                st.toast(f"â³ ä¼ºæœå™¨å¿™ç¢Œï¼Œ{wait}ç§’å¾Œé‡è©¦...", icon="âš ï¸")
                time.sleep(wait)
            else: raise e
    raise Exception("é‡è©¦æ¬¡æ•¸éå¤š")

# --- 4. ä»‹é¢è¦–è¦ºè¨­è¨ˆ ---
st.set_page_config(page_title="å…§æ¹–åœ‹å° AI è¼”åŠ©å‡ºé¡Œç³»çµ±", layout="wide")

st.markdown("""
    <style>
    header[data-testid="stHeader"], footer { display: none !important; }
    .stApp { background-color: #0F172A; }
    .block-container { max-width: 1200px; padding-top: 1.5rem !important; }
    .school-header { background: linear-gradient(90deg, #1E293B 0%, #334155 100%); padding: 25px; border-radius: 15px; text-align: center; margin-bottom: 25px; border: 1px solid #475569; }
    .school-name { font-size: 24px; font-weight: 700; color: #F1F5F9; letter-spacing: 3px; }
    .app-title { font-size: 14px; color: #94A3B8; }
    .comfort-box { background-color: #1E293B; padding: 12px; border-radius: 10px; margin-bottom: 12px; border-left: 5px solid #3B82F6; font-size: 13px; color: #CBD5E1; line-height: 1.6; }
    .comfort-box a { color: #60A5FA !important; text-decoration: none; font-weight: bold; }
    [data-testid="stSidebar"] .stButton > button { width: 100%; height: 40px; }
    .custom-footer { position: fixed; left: 0; bottom: 0; width: 100%; background-color: #0F172A; color: #475569; text-align: center; padding: 10px; font-size: 11px; z-index: 100; }
    </style>
    <div class="school-header">
        <div class="school-name">æ–°ç«¹å¸‚é¦™å±±å€å…§æ¹–åœ‹å°</div>
        <div class="app-title">è©•é‡å‘½é¡Œèˆ‡å­¸ç¿’ç›®æ¨™è‡ªå‹•åŒ–ç³»çµ±</div>
    </div>
    """, unsafe_allow_html=True)

# ç‹€æ…‹
if "phase" not in st.session_state: st.session_state.phase = 1 
if "chat_history" not in st.session_state: st.session_state.chat_history = []
if "last_prompt_content" not in st.session_state: st.session_state.last_prompt_content = ""

# --- Sidebar ---
with st.sidebar:
    st.markdown("### ğŸš€ å¿«é€ŸæŒ‡å—")
    st.markdown("""<div class="comfort-box"><ol style="margin:0; padding-left:1.2rem;">
        <li>å‰å¾€ <a href="https://aistudio.google.com/" target="_blank">AI Studio (é»æˆ‘)</a></li>
        <li>ç™»å…¥<b>å€‹äºº Google å¸³è™Ÿ</b></li>
        <li>é»æ“Š <b>Get API key</b> ä¸¦è¤‡è£½è²¼å…¥ä¸‹æ–¹</li></ol></div>""", unsafe_allow_html=True)
    api_input = st.text_area("åœ¨æ­¤è¼¸å…¥ API Key", height=70)
    if st.button("ğŸ”„ é‡ç½®ç³»çµ±"):
        for k in ["phase", "chat_history", "last_prompt_content"]: st.session_state[k] = (1 if k=="phase" else [] if k=="chat_history" else "")
        st.rerun()
    st.markdown("### ğŸ“š è³‡æºé€£çµ")
    st.markdown("""<div class="comfort-box"><b>æ•™æï¼š</b><a href="https://webetextbook.knsh.com.tw/" target="_blank">åº·è»’</a> | <a href="https://edisc3.hle.com.tw/" target="_blank">ç¿°æ—</a> | <a href="https://reader.nani.com.tw/" target="_blank">å—ä¸€</a><br><b>åƒè€ƒï¼š</b><a href="https://cirn.moe.edu.tw/Syllabus/index.aspx?sid=1108" target="_blank">108èª²ç¶±</a> | <a href="https://www.nhps.hc.edu.tw/" target="_blank">æ ¡ç¶²</a></div>""", unsafe_allow_html=True)

# --- Phase 1: åƒæ•¸è¨­å®šèˆ‡æ•™æä¸Šå‚³ ---
if st.session_state.phase == 1:
    with st.container(border=True):
        st.markdown("### ğŸ“ ç¬¬ä¸€éšæ®µï¼šåƒæ•¸è¨­å®šèˆ‡æ•™æä¸Šå‚³")
        c1, c2, c3 = st.columns(3)
        with c1: grade = st.selectbox("1. å¹´ç´š", ["", "ä¸€å¹´ç´š", "äºŒå¹´ç´š", "ä¸‰å¹´ç´š", "å››å¹´ç´š", "äº”å¹´ç´š", "å…­å¹´ç´š"])
        with c2: subject = st.selectbox("2. ç§‘ç›®", ["", "åœ‹èª", "æ•¸å­¸", "è‡ªç„¶ç§‘å­¸", "ç¤¾æœƒ", "è‹±èª"])
        with c3: mode = st.selectbox("3. æ¨¡å¼", ["ğŸŸ¢ é©ä¸­", "ğŸ”´ å›°é›£", "ğŸŒŸ ç´ é¤Š"])
        
        st.markdown("**4. å‹¾é¸æ¬²ç”¢å‡ºçš„é¡Œå‹**")
        available_types = SUBJECT_Q_TYPES.get(subject, SUBJECT_Q_TYPES[""])
        cols = st.columns(min(len(available_types), 4))
        selected_types = [t for i, t in enumerate(available_types) if cols[i % len(cols)].checkbox(t, value=True)]
        
        uploaded_files = st.file_uploader("5. ä¸Šå‚³æ•™æ (Word/PDF)", type=["pdf", "docx", "doc"], accept_multiple_files=True)
        
        if st.button("ğŸš€ ç”¢å‡ºå­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨", type="primary", use_container_width=True):
            if not api_input or not grade or not subject or not uploaded_files:
                st.warning("âš ï¸ è«‹è£œé½Š API Keyã€åƒæ•¸æˆ–æ•™æã€‚")
            else:
                with st.spinner("âš¡ æ­£åœ¨æœå°‹å¯ç”¨æ¨¡å‹ä¸¦åˆ†ææ•™æ..."):
                    target_key = api_input.strip()
                    # é—œéµä¿®æ­£ï¼šè‡ªå‹•å°‹æ‰¾æ­£ç¢ºçš„æ¨¡å‹å­—ä¸²
                    model_name, error = find_available_model(target_key, "flash")
                    
                    if error:
                        st.error(f"âŒ ç„¡æ³•è®€å–æ¨¡å‹æ¸…å–®ï¼Œè«‹ç¢ºèª API Key æ˜¯å¦æœ‰æ•ˆï¼š{error}")
                    else:
                        content = extract_text_from_files(uploaded_files)
                        try:
                            st.toast(f"âœ… å·²åµæ¸¬å¯ç”¨æ¨¡å‹ï¼š{model_name}")
                            model = genai.GenerativeModel(model_name, system_instruction="ä½ åƒ…ç”¢å‡ºè¡¨æ ¼ï¼Œæ¬„ä½ï¼š| å–®å…ƒåç¨± | å­¸ç¿’ç›®æ¨™(åŸæ–‡) | å°æ‡‰é¡Œå‹ | é è¨ˆé…åˆ† |ã€‚çµ•å°ç¦æ­¢å‡ºé¡Œï¼")
                            chat = model.start_chat(history=[])
                            prompt = f"å¹´ç´šï¼š{grade}, ç§‘ç›®ï¼š{subject}\né¡Œå‹ï¼š{'ã€'.join(selected_types)}\nå‘½é¡Œæ¨¡å¼ï¼š{mode}\næ•™æï¼š{content}"
                            st.session_state.last_prompt_content = prompt
                            
                            with st.chat_message("ai"):
                                placeholder = st.empty()
                                full_res = ""
                                res = generate_with_retry(chat, prompt)
                                for chunk in res:
                                    full_res += chunk.text
                                    placeholder.markdown(full_res + "â–Œ")
                                placeholder.markdown(full_res)
                            
                            st.session_state.chat_history.append({"role": "model", "content": full_res})
                            st.session_state.phase = 2
                            st.rerun()
                        except Exception as e: st.error(f"é€£ç·šå¤±æ•—ï¼š{e}")

# --- Phase 2: ç¢ºèªèˆ‡å‡ºé¡Œ ---
elif st.session_state.phase == 2:
    current_md = st.session_state.chat_history[0]["content"]
    with st.chat_message("ai"): st.markdown(current_md)
    
    df = process_table_data(current_md)
    if df is not None:
        c_d1, c_d2 = st.columns(2)
        with c_d1:
            try:
                buf = io.BytesIO()
                with pd.ExcelWriter(buf, engine='openpyxl') as writer: df.to_excel(writer, index=False)
                st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel å¯©æ ¸è¡¨", data=buf.getvalue(), file_name="å¯©æ ¸è¡¨.xlsx", use_container_width=True)
            except: st.caption("ç’°å¢ƒä¸æ”¯æ´ Excelï¼Œè«‹ç”¨ CSVã€‚")
        with c_d2:
            st.download_button("ğŸ“¥ ä¸‹è¼‰ CSV å¯©æ ¸è¡¨ (ä¿éšªç”¨)", data=df.to_csv(index=False).encode('utf-8-sig'), file_name="å¯©æ ¸è¡¨.csv", use_container_width=True)

    st.divider()
    with st.container(border=True):
        st.markdown("### ğŸ“ ç¬¬äºŒéšæ®µï¼šæ­£å¼å‡ºé¡Œ")
        if st.button("âœ… ç¢ºèªç„¡èª¤ï¼Œé–‹å§‹å‡ºé¡Œ", type="primary", use_container_width=True):
            with st.spinner("ğŸ§  æ­£åœ¨è‡ªå‹•åŒ¹é…æœ€å¼·æ¨¡å‹ä¸¦å‘½é¡Œä¸­..."):
                target_key = api_input.strip()
                # è‡ªå‹•æ‰¾ Pro æ¨¡å‹
                model_name_pro, error = find_available_model(target_key, "pro")
                
                if model_name_pro:
                    st.toast(f"ğŸ§  åˆ‡æ›è‡³æ——è‰¦å¤§è…¦ï¼š{model_name_pro}")
                    model_pro = genai.GenerativeModel(model_name_pro, system_instruction="è«‹æ ¹æ“šå¯©æ ¸è¡¨ç”¢å‡ºæ­£å¼è©¦å·èˆ‡åƒè€ƒç­”æ¡ˆã€‚")
                    with st.chat_message("ai"):
                        placeholder = st.empty()
                        full_res = ""
                        res = generate_with_retry(model_pro, f"{st.session_state.last_prompt_content}\n---\nåƒè€ƒå¯©æ ¸è¡¨ï¼š\n{current_md}\n\nè«‹æ­£å¼å‡ºé¡Œã€‚")
                        for chunk in res:
                            full_res += chunk.text
                            placeholder.markdown(full_res + "â–Œ")
                        placeholder.markdown(full_res)
                    st.session_state.chat_history.append({"role": "model", "content": full_res})

        if st.button("â¬…ï¸ è¿”å›ä¿®æ”¹åƒæ•¸", use_container_width=True):
            st.session_state.phase = 1
            st.rerun()
    
    if len(st.session_state.chat_history) > 1:
        for msg in st.session_state.chat_history[1:]:
             with st.chat_message("ai"): st.markdown(msg["content"])

st.markdown('<div class="custom-footer">Â© 2026 æ–°ç«¹å¸‚é¦™å±±å€å…§æ¹–åœ‹å°. All Rights Reserved.</div>', unsafe_allow_html=True)
