import streamlit as st
import google.generativeai as genai
import random
import io
import time
import re # å¼•å…¥æ­£è¦è¡¨ç¤ºå¼
from pypdf import PdfReader
from docx import Document
import pandas as pd
import subprocess
import os

# --- 1. å®šç¾©å­¸ç§‘èˆ‡é¡Œå‹æ˜ å°„ ---
SUBJECT_Q_TYPES = {
Â  Â  "åœ‹èª": ["åœ‹å­—æ³¨éŸ³", "é€ å¥", "å–®é¸é¡Œ", "é–±è®€ç´ é¤Šé¡Œ", "å¥å‹è®Šæ›", "ç°¡ç­”é¡Œ"],
Â  Â  "æ•¸å­¸": ["æ‡‰ç”¨è¨ˆç®—é¡Œ", "åœ–è¡¨åˆ†æé¡Œ", "å¡«å……é¡Œ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ"],
Â  Â  "è‡ªç„¶ç§‘å­¸": ["å¯¦é©—åˆ¤è®€é¡Œ", "åœ–è¡¨åˆ†æé¡Œ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ", "å¡«å……é¡Œ", "é…åˆé¡Œ"],
Â  Â  "ç¤¾æœƒ": ["åœ°åœ–åˆ¤è®€é¡Œ", "æƒ…å¢ƒæ¡ˆä¾‹åˆ†æ", "å–®é¸é¡Œ", "æ˜¯éé¡Œ", "é…åˆé¡Œ", "ç°¡ç­”é¡Œ"],
Â  Â  "è‹±èª": ["è‹±èªæœƒè©±é¸æ“‡", "è©å½™æ­é…", "æ–‡æ„é¸å¡«", "å–®é¸é¡Œ", "é–±è®€ç†è§£"],
Â  Â  "": ["å–®é¸é¡Œ", "æ˜¯éé¡Œ", "å¡«å……é¡Œ", "ç°¡ç­”é¡Œ"]
}

# --- 2. æª”æ¡ˆè®€å–å·¥å…· (å¿«å–å„ªåŒ–) ---
@st.cache_data
def extract_text_from_files(files):
Â  Â  text_content = ""
Â  Â  for file in files:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  ext = file.name.split('.')[-1].lower()
Â  Â  Â  Â  Â  Â  if ext == 'pdf':
Â  Â  Â  Â  Â  Â  Â  Â  pdf_reader = PdfReader(file)
Â  Â  Â  Â  Â  Â  Â  Â  text_content += "".join([p.extract_text() or "" for p in pdf_reader.pages])
Â  Â  Â  Â  Â  Â  elif ext == 'docx':
Â  Â  Â  Â  Â  Â  Â  Â  doc = Document(file)
Â  Â  Â  Â  Â  Â  Â  Â  text_content += "\n".join([p.text for p in doc.paragraphs])
Â  Â  Â  Â  Â  Â  elif ext == 'doc':
Â  Â  Â  Â  Â  Â  Â  Â  with open("temp.doc", "wb") as f: f.write(file.getbuffer())
Â  Â  Â  Â  Â  Â  Â  Â  result = subprocess.run(['antiword', 'temp.doc'], capture_output=True, text=True)
Â  Â  Â  Â  Â  Â  Â  Â  if result.returncode == 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text_content += result.stdout
Â  Â  Â  Â  Â  Â  Â  Â  if os.path.exists("temp.doc"): os.remove("temp.doc")
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  text_content += f"\n[è®€å–éŒ¯èª¤: {file.name}]"
Â  Â  return text_content

# --- 3. Excel ä¸‹è¼‰å·¥å…· (æ™ºæ…§åˆ‡åˆ†çµ‚æ¥µç‰ˆ) --- [cite: 2026-02-13]
def md_to_excel(md_text):
Â  Â  try:
Â  Â  Â  Â  # 1. é è™•ç†ï¼šç¬¦è™Ÿæ­£è¦åŒ–
Â  Â  Â  Â  # å°‡å…¨å½¢ï½œè½‰ç‚ºåŠå½¢ |ï¼Œç§»é™¤ Markdown ç²—é«”ç¬¦è™Ÿ **
Â  Â  Â  Â  cleaned_text = md_text.replace("ï½œ", "|").replace("**", "")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 2. å°‹æ‰¾æ¨™é¡Œåˆ— (Anchor)
Â  Â  Â  Â  # ä¸ç®¡æœ‰ç„¡æ›è¡Œï¼Œå…ˆå˜—è©¦ç”¨ regex æŠ“å‡ºæ¨™é¡Œå€å¡Š
Â  Â  Â  Â  # æ¨™é¡Œç‰¹å¾µï¼š| å–®å…ƒåç¨± | ... | é è¨ˆé…åˆ† |
Â  Â  Â  Â  header_match = re.search(r'\|\s*å–®å…ƒåç¨±\s*\|\s*å­¸ç¿’ç›®æ¨™.*\|\s*å°æ‡‰é¡Œå‹\s*\|\s*é è¨ˆé…åˆ†\s*\|', cleaned_text)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not header_match:
Â  Â  Â  Â  Â  Â  return None, "âŒ æ‰¾ä¸åˆ°è¡¨æ ¼æ¨™é¡Œåˆ— (éœ€åŒ…å«ï¼šå–®å…ƒåç¨±ã€å­¸ç¿’ç›®æ¨™...)"

Â  Â  Â  Â  # 3. æå–æ‰€æœ‰å„²å­˜æ ¼è³‡æ–™
Â  Â  Â  Â  # å¾æ¨™é¡Œé–‹å§‹ï¼ŒæŠ“å–å¾ŒçºŒæ‰€æœ‰é€é | åˆ†éš”çš„å…§å®¹
Â  Â  Â  Â  start_index = header_match.start()
Â  Â  Â  Â  table_content = cleaned_text[start_index:]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # ä½¿ç”¨ split('|') å°‡æ‰€æœ‰å…§å®¹åˆ‡æˆç¢ç‰‡
Â  Â  Â  Â  # éæ¿¾æ‰ç©ºå­—ä¸²ã€æ›è¡Œç¬¦ã€åˆ†éš”ç·š(---)
Â  Â  Â  Â  raw_cells = [
Â  Â  Â  Â  Â  Â  c.strip()Â 
Â  Â  Â  Â  Â  Â  for c in table_content.split('|')Â 
Â  Â  Â  Â  Â  Â  if c.strip() and '---' not in c
Â  Â  Â  Â  ]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 4. æ™ºæ…§é‡çµ„ (Smart Chunking)
Â  Â  Â  Â  # æˆ‘å€‘çŸ¥é“æ¨™æº–è¡¨æ ¼æ˜¯ 4 å€‹æ¬„ä½ (å–®å…ƒ, ç›®æ¨™, é¡Œå‹, é…åˆ†)
Â  Â  Â  Â  num_cols = 4Â 
Â  Â  Â  Â Â 
Â  Â  Â  Â  # æª¢æŸ¥æ¬„ä½æ•¸æ˜¯å¦æ­£ç¢º
Â  Â  Â  Â  if len(raw_cells) < num_cols:
Â  Â  Â  Â  Â  Â  return None, f"âŒ è³‡æ–™é‡ä¸è¶³ (åƒ… {len(raw_cells)} å€‹æ¬„ä½)"

Â  Â  Â  Â  headers = raw_cells[:num_cols] # å‰ 4 å€‹æ˜¯æ¨™é¡Œ
Â  Â  Â  Â  data_cells = raw_cells[num_cols:] # å¾Œé¢å…¨æ˜¯è³‡æ–™
Â  Â  Â  Â Â 
Â  Â  Â  Â  # å°‡è³‡æ–™æ¯ 4 å€‹ä¸€çµ„åˆ‡åˆ†
Â  Â  Â  Â  rows = []
Â  Â  Â  Â  for i in range(0, len(data_cells), num_cols):
Â  Â  Â  Â  Â  Â  row = data_cells[i : i + num_cols]
Â  Â  Â  Â  Â  Â  # å¦‚æœæœ€å¾Œä¸€åˆ—ä¸æ»¿ 4 å€‹ï¼Œè£œç©ºå€¼
Â  Â  Â  Â  Â  Â  if len(row) < num_cols:
Â  Â  Â  Â  Â  Â  Â  Â  row += [''] * (num_cols - len(row))
Â  Â  Â  Â  Â  Â  rows.append(row)

Â  Â  Â  Â  if not rows:
Â  Â  Â  Â  Â  Â  return None, "âŒ è¡¨æ ¼å…§ç„¡è³‡æ–™"

Â  Â  Â  Â  # 5. è½‰æˆ DataFrame
Â  Â  Â  Â  df = pd.DataFrame(rows, columns=headers)
Â  Â  Â  Â Â 
Â  Â  Â  Â  output = io.BytesIO()
Â  Â  Â  Â  with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
Â  Â  Â  Â  Â  Â  df.to_excel(writer, index=False, sheet_name='å­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨')
Â  Â  Â  Â  Â  Â  worksheet = writer.sheets['å­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨']
Â  Â  Â  Â  Â  Â  for i, col in enumerate(df.columns):
Â  Â  Â  Â  Â  Â  Â  Â  worksheet.set_column(i, i, 25)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  return output.getvalue(), None # æˆåŠŸå›å‚³ (data, error=None)

Â  Â  except Exception as e:
Â  Â  Â  Â  return None, f"âŒ ç¨‹å¼è½‰æ›éŒ¯èª¤: {str(e)}"

# --- 4. æ ¸å¿ƒ Gem å‘½é¡Œéµå¾‹ ---
GEM_INSTRUCTIONS = """
ä½ æ˜¯ã€Œåœ‹å°å°ˆæ¥­å®šæœŸè©•é‡å‘½é¡Œ AIã€ã€‚

### âš ï¸ Phase 1 çµ•å°è¦å‰‡ï¼š
1. **ä»»å‹™**ï¼šåƒ…ç”¢å‡ºã€å­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨ã€‘ã€‚
2. **ç¦æ­¢**ï¼š
Â  Â - âŒ åš´ç¦ç”¢å‡ºè©¦é¡Œã€‚
Â  Â - âŒ åš´ç¦ä»»ä½•å‰è¨€æˆ–çµèªã€‚
3. **æ ¼å¼**ï¼š
Â  Â - è«‹ä½¿ç”¨æ¨™æº– Markdown è¡¨æ ¼ã€‚
Â  Â - æ¬„ä½é †åºå¿…é ˆç‚ºï¼š| å–®å…ƒåç¨± | å­¸ç¿’ç›®æ¨™(åŸæ–‡) | å°æ‡‰é¡Œå‹ | é è¨ˆé…åˆ† |
Â  Â - **è«‹ç¢ºä¿æ¯å€‹å„²å­˜æ ¼å…§å®¹ä¸è¦åŒ…å«æ›è¡Œç¬¦è™Ÿ**ï¼Œä»¥å…è¡¨æ ¼ç ´è£‚ã€‚
"""

# --- 5. æ™ºèƒ½æ¨¡å‹é¸æ“‡èˆ‡é‡è©¦æ©Ÿåˆ¶ ---
def get_best_model(api_key, mode="fast"):
Â  Â  genai.configure(api_key=api_key)
Â  Â  try:
Â  Â  Â  Â  models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
Â  Â  Â  Â  if not models: return None, "æ‰¾ä¸åˆ°å¯ç”¨æ¨¡å‹"
Â  Â  Â  Â  target_model = None
Â  Â  Â  Â  if mode == "fast":
Â  Â  Â  Â  Â  Â  for m in models:
Â  Â  Â  Â  Â  Â  Â  Â  if 'flash' in m.lower(): target_model = m; break
Â  Â  Â  Â  Â  Â  if not target_model: target_model = models[0]
Â  Â  Â  Â  elif mode == "smart":
Â  Â  Â  Â  Â  Â  for m in models:
Â  Â  Â  Â  Â  Â  Â  Â  if 'pro' in m.lower() and '1.5' in m.lower(): target_model = m; break
Â  Â  Â  Â  Â  Â  if not target_model:
Â  Â  Â  Â  Â  Â  Â  Â  for m in models:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'pro' in m.lower(): target_model = m; break
Â  Â  Â  Â  if not target_model: target_model = models[0]
Â  Â  Â  Â  return target_model, None
Â  Â  except Exception as e: return None, str(e)

def generate_with_retry(model_or_chat, prompt, stream=True):
Â  Â  max_retries = 3
Â  Â  for i in range(max_retries):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  if hasattr(model_or_chat, 'send_message'):
Â  Â  Â  Â  Â  Â  Â  Â  return model_or_chat.send_message(prompt, stream=stream)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  return model_or_chat.generate_content(prompt, stream=stream)
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  if "429" in str(e):
Â  Â  Â  Â  Â  Â  Â  Â  wait_time = (i + 1) * 5
Â  Â  Â  Â  Â  Â  Â  Â  st.toast(f"â³ ä¼ºæœå™¨å¿™ç¢Œ (429)ï¼Œ{wait_time} ç§’å¾Œè‡ªå‹•é‡è©¦ ({i+1}/{max_retries})...", icon="âš ï¸")
Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(wait_time)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  raise e
Â  Â  raise Exception("é‡è©¦æ¬¡æ•¸éå¤šï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")

# --- 6. ç¶²é ä»‹é¢è¦–è¦ºè¨­è¨ˆ ---
st.set_page_config(page_title="å…§æ¹–åœ‹å° AI è¼”åŠ©å‡ºé¡Œç³»çµ±", layout="wide")

st.markdown("""
Â  Â  <style>
Â  Â  header[data-testid="stHeader"] { display: none !important; visibility: hidden !important; }
Â  Â  footer { display: none !important; visibility: hidden !important; }

Â  Â  .stApp { background-color: #0F172A; }
Â  Â  .block-container { max-width: 1200px; padding-top: 1.5rem !important; padding-bottom: 5rem; }
Â  Â Â 
Â  Â  .school-header {
Â  Â  Â  Â  background: linear-gradient(90deg, #1E293B 0%, #334155 100%);
Â  Â  Â  Â  padding: 25px; border-radius: 18px; text-align: center; margin-bottom: 25px;Â 
Â  Â  Â  Â  border: 1px solid #475569;
Â  Â  }
Â  Â  .school-name { font-size: 26px; font-weight: 700; color: #F1F5F9; letter-spacing: 3px; }
Â  Â  .app-title { font-size: 15px; color: #94A3B8; margin-top: 6px; }
Â  Â  h1, h2, h3, p, span, label, .stMarkdown { color: #E2E8F0 !important; }
Â  Â Â 
Â  Â  .comfort-box {
Â  Â  Â  Â  background-color: #1E293B; padding: 15px; border-radius: 10px;Â 
Â  Â  Â  Â  margin-bottom: 15px; border-left: 5px solid #3B82F6;Â 
Â  Â  Â  Â  font-size: 14px; color: #CBD5E1; line-height: 1.8;
Â  Â  }
Â  Â  .comfort-box b { color: #fff; }
Â  Â  .comfort-box a { color: #60A5FA !important; text-decoration: none; font-weight: bold; }
Â  Â Â 
Â  Â  [data-testid="stSidebar"] .stMarkdown { margin-bottom: 10px; }Â 
Â  Â  .stTextArea textarea { min-height: 80px; }
Â  Â  .stTextArea { margin-bottom: 15px !important; }
Â  Â  [data-testid="stSidebar"] .stButton > button {Â 
Â  Â  Â  Â  display: block; margin: 15px auto !important;Â 
Â  Â  Â  Â  width: 100%; border-radius: 8px; height: 42px;
Â  Â  Â  Â  background-color: #334155; border: 1px solid #475569; font-size: 15px;
Â  Â  }
Â  Â Â 
Â  Â  .custom-footer {Â 
Â  Â  Â  Â  position: fixed; left: 0; bottom: 0; width: 100%;Â 
Â  Â  Â  Â  background-color: #0F172A; color: #475569;Â 
Â  Â  Â  Â  text-align: center; padding: 12px; font-size: 11px;Â 
Â  Â  Â  Â  border-top: 1px solid #1E293B; z-index: 100;Â 
Â  Â  }
Â  Â  </style>
Â  Â Â 
Â  Â  <div class="school-header">
Â  Â  Â  Â  <div class="school-name">æ–°ç«¹å¸‚é¦™å±±å€å…§æ¹–åœ‹å°</div>
Â  Â  Â  Â  <div class="app-title">è©•é‡å‘½é¡Œèˆ‡å­¸ç¿’ç›®æ¨™è‡ªå‹•åŒ–ç³»çµ±</div>
Â  Â  </div>
Â  Â  """, unsafe_allow_html=True)

# ç‹€æ…‹ç®¡ç†
if "phase" not in st.session_state: st.session_state.phase = 1Â 
if "chat_history" not in st.session_state: st.session_state.chat_history = []
if "last_prompt_content" not in st.session_state: st.session_state.last_prompt_content = ""

# --- Sidebar ---
with st.sidebar:
Â  Â  st.markdown("### ğŸš€ å¿«é€ŸæŒ‡å—")
Â  Â  st.markdown("""
Â  Â  <div class="comfort-box">
Â  Â  Â  Â  <ol style="margin:0; padding-left:1.2rem;">
Â  Â  Â  Â  Â  Â  <li>å‰å¾€ <a href="https://aistudio.google.com/" target="_blank">Google AI Studio (é»æˆ‘)</a></li>
Â  Â  Â  Â  Â  Â  <li>ç™»å…¥<b>å€‹äºº Google å¸³è™Ÿ</b> (é¿é–‹æ•™è‚²ç‰ˆ)</li>
Â  Â  Â  Â  Â  Â  <li>é»æ“Š <b>Get API key</b> ä¸¦è¤‡è£½</li>
Â  Â  Â  Â  Â  Â  <li>è²¼å…¥ä¸‹æ–¹æ¬„ä½</li>
Â  Â  Â  Â  </ol>
Â  Â  </div>
Â  Â  """, unsafe_allow_html=True)
Â  Â Â 
Â  Â  api_input = st.text_area("åœ¨æ­¤è¼¸å…¥ API Key", height=80, placeholder="è«‹è²¼ä¸Šé‡‘é‘°...")
Â  Â Â 
Â  Â  if st.button("ğŸ”„ é‡ç½®ç³»çµ±"):
Â  Â  Â  Â  st.session_state.phase = 1
Â  Â  Â  Â  st.session_state.chat_history = []
Â  Â  Â  Â  st.session_state.last_prompt_content = ""
Â  Â  Â  Â  st.rerun()

Â  Â  st.markdown("### ğŸ“š è³‡æºé€£çµ")
Â  Â  st.markdown("""
Â  Â  <div class="comfort-box">
Â  Â  Â  Â  <b>æ•™æä¸‹è¼‰ï¼š</b><br>
Â  Â  Â  Â  â€¢ <a href="https://webetextbook.knsh.com.tw/" target="_blank">åº·è»’é›»å­æ›¸</a><br>
Â  Â  Â  Â  â€¢ <a href="https://edisc3.hle.com.tw/" target="_blank">ç¿°æ—è¡Œå‹•å¤§å¸«</a><br>
Â  Â  Â  Â  â€¢ <a href="https://reader.nani.com.tw/" target="_blank">å—ä¸€ OneBox</a><br>
Â  Â  Â  Â  <br>
Â  Â  Â  Â  <b>åƒè€ƒè³‡æ–™ï¼š</b><br>
Â  Â  Â  Â  â€¢ <a href="https://cirn.moe.edu.tw/Syllabus/index.aspx?sid=1108" target="_blank">108èª²ç¶±è³‡æºç¶² (CIRN)</a><br>
Â  Â  Â  Â  â€¢ <a href="https://www.nhps.hc.edu.tw/" target="_blank">å…§æ¹–åœ‹å°æ ¡ç¶²</a>
Â  Â  </div>
Â  Â  """, unsafe_allow_html=True)

# --- Phase 1: åƒæ•¸è¨­å®šèˆ‡æ•™æä¸Šå‚³ ---
if st.session_state.phase == 1:
Â  Â  with st.container(border=True):
Â  Â  Â  Â  st.markdown("### ğŸ“ ç¬¬ä¸€éšæ®µï¼šåƒæ•¸è¨­å®šèˆ‡æ•™æä¸Šå‚³")
Â  Â  Â  Â Â 
Â  Â  Â  Â  c1, c2, c3 = st.columns(3)
Â  Â  Â  Â  with c1: grade = st.selectbox("1. é¸æ“‡å¹´ç´š", ["", "ä¸€å¹´ç´š", "äºŒå¹´ç´š", "ä¸‰å¹´ç´š", "å››å¹´ç´š", "äº”å¹´ç´š", "å…­å¹´ç´š"], index=0)
Â  Â  Â  Â  with c2: subject = st.selectbox("2. é¸æ“‡ç§‘ç›®", ["", "åœ‹èª", "æ•¸å­¸", "è‡ªç„¶ç§‘å­¸", "ç¤¾æœƒ", "è‹±èª"], index=0)
Â  Â  Â  Â  with c3: mode = st.selectbox("3. å‘½é¡Œæ¨¡å¼", ["ğŸŸ¢ æ¨¡å¼ Aï¼šé©ä¸­", "ğŸ”´ æ¨¡å¼ Bï¼šå›°é›£", "ğŸŒŸ æ¨¡å¼ Cï¼šç´ é¤Š"], index=0)
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  st.markdown("**4. å‹¾é¸æ¬²ç”¢å‡ºçš„é¡Œå‹**")
Â  Â  Â  Â  available_types = SUBJECT_Q_TYPES.get(subject, SUBJECT_Q_TYPES[""])
Â  Â  Â  Â  cols = st.columns(min(len(available_types), 4))
Â  Â  Â  Â  selected_types = []
Â  Â  Â  Â  for i, t in enumerate(available_types):
Â  Â  Â  Â  Â  Â  if cols[i % len(cols)].checkbox(t, value=True):
Â  Â  Â  Â  Â  Â  Â  Â  selected_types.append(t)
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  uploaded_files = st.file_uploader("5. ä¸Šå‚³æ•™ææª”æ¡ˆ (Word/PDF)", type=["pdf", "docx", "doc"], accept_multiple_files=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if st.button("ğŸš€ ç”¢å‡ºå­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨", type="primary", use_container_width=True):
Â  Â  Â  Â  Â  Â  if not api_input:
Â  Â  Â  Â  Â  Â  Â  Â  st.error("âŒ å‹•ä½œä¸­æ­¢ï¼šå´é‚Šæ¬„å°šæœªè¼¸å…¥ API Keyã€‚")
Â  Â  Â  Â  Â  Â  elif not grade or not subject or not uploaded_files or not selected_types:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning("âš ï¸ å‹•ä½œä¸­æ­¢ï¼šè«‹ç¢ºèªå¹´ç´šã€ç§‘ç›®ã€é¡Œå‹èˆ‡æ•™æå·²å‚™å¦¥ã€‚")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("âš¡ æ­£åœ¨æ¥µé€Ÿæƒææ•™æå…§å®¹ï¼Œè«‹ç¨å€™..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keys = [k.strip() for k in api_input.replace('\n', ',').split(',') if k.strip()]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  target_key = random.choice(keys)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model_name, error_msg = get_best_model(target_key, mode="fast")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if error_msg:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ API é€£ç·šéŒ¯èª¤ï¼š{error_msg}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content = extract_text_from_files(uploaded_files)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.toast(f"âš¡ å•Ÿå‹• AI å¼•æ“ ({model_name}) åˆ†æä¸­...", icon="ğŸ¤–")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model_fast = genai.GenerativeModel(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model_name=model_name,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  system_instruction=GEM_INSTRUCTIONS,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  generation_config={"temperature": 0.0}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chat = model_fast.start_chat(history=[])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.chat_message("ai"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message_placeholder = st.empty()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_response = ""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  t_str = "ã€".join(selected_types)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prompt_content = f"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ä»»å‹™ï¼šPhase 1 å­¸ç¿’ç›®æ¨™æå–
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  å¹´ç´šï¼š{grade}, ç§‘ç›®ï¼š{subject}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  é¡Œå‹ï¼š{t_str}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  æ•™æå…§å®¹ï¼š
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {content}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  è«‹ç”¢å‡ºã€å­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨ã€‘ã€‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **âš ï¸ åš´æ ¼æ ¼å¼è¦æ±‚ï¼š**
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  1. åƒ…ç”¢å‡ºè¡¨æ ¼ï¼Œ**åš´ç¦**ç”¢å‡ºè©¦é¡Œã€‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  2. è¡¨æ ¼æ¨™é¡Œè¡Œå¿…é ˆåŒ…å«ï¼š| å–®å…ƒåç¨± | å­¸ç¿’ç›®æ¨™(åŸæ–‡) | å°æ‡‰é¡Œå‹ | é è¨ˆé…åˆ† |
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  3. **è«‹ç¢ºä¿æ¯å€‹å„²å­˜æ ¼å…§å®¹ä¸è¦åŒ…å«æ›è¡Œç¬¦è™Ÿ**ã€‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.last_prompt_content = prompt_content
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = generate_with_retry(chat, prompt_content, stream=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for chunk in response:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_response += chunk.text
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message_placeholder.markdown(full_response + "â–Œ")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message_placeholder.markdown(full_response)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "ERROR_SUBJECT_MISMATCH" in full_response:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ é˜²å‘†å•Ÿå‹•ï¼šæ•™æå…§å®¹èˆ‡ã€{subject}ã€ä¸ç¬¦ï¼Œè«‹é‡æ–°ç¢ºèªæª”æ¡ˆã€‚")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append({"role": "model", "content": full_response})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.phase = 2
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"é€£ç·šå¤±æ•—ï¼š{e}")

# --- Phase 2: æ­£å¼å‡ºé¡Œ ---
elif st.session_state.phase == 2:
Â  Â  current_md = st.session_state.chat_history[0]["content"]
Â  Â Â 
Â  Â  with st.container(border=True):
Â  Â  Â  Â  st.markdown("### ğŸ“¥ ç¬¬äºŒéšæ®µï¼šä¸‹è¼‰å¯©æ ¸è¡¨")
Â  Â  Â  Â  with st.chat_message("ai"): st.markdown(current_md)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # å‘¼å«æ–°çš„è½‰æ›å‡½æ•¸ï¼Œä¸¦æ¥æ”¶éŒ¯èª¤è¨Šæ¯
Â  Â  Â  Â  excel_data, error_msg = md_to_excel(current_md)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if excel_data:
Â  Â  Â  Â  Â  Â  st.download_button(label="ğŸ“¥ åŒ¯å‡ºæ­¤å¯©æ ¸è¡¨ (Excel)", data=excel_data, file_name=f"å…§æ¹–åœ‹å°_{subject}_å¯©æ ¸è¡¨.xlsx", use_container_width=True)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  # é¡¯ç¤ºå…·é«”éŒ¯èª¤åŸå› 
Â  Â  Â  Â  Â  Â  st.error(f"âš ï¸ è¡¨æ ¼è½‰æ›å¤±æ•—ï¼š{error_msg}")
Â  Â  Â  Â  Â  Â  with st.expander("ğŸ” æŸ¥çœ‹ AI åŸå§‹è¼¸å‡º (Debug)"):
Â  Â  Â  Â  Â  Â  Â  Â  st.text(current_md)

Â  Â  st.divider()
Â  Â  with st.container(border=True):
Â  Â  Â  Â  st.markdown("### ğŸ“ ç¬¬ä¸‰éšæ®µï¼šè©¦å·æ­£å¼ç”Ÿæˆ")
Â  Â  Â  Â Â 
Â  Â  Â  Â  cb1, cb2 = st.columns(2)
Â  Â  Â  Â  with cb1:
Â  Â  Â  Â  Â  Â  if st.button("âœ… å¯©æ ¸è¡¨ç¢ºèªç„¡èª¤ï¼Œé–‹å§‹å‡ºé¡Œ", type="primary", use_container_width=True):
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("ğŸ§  æ­£åœ¨é€²è¡Œæ·±åº¦æ¨ç†å‘½é¡Œï¼Œè«‹ç¨å€™..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keys = [k.strip() for k in api_input.replace('\n', ',').split(',') if k.strip()]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  target_key = random.choice(keys)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model_name, error_msg = get_best_model(target_key, mode="smart")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if error_msg:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.error(f"âŒ ç„¡æ³•å•Ÿå‹•é«˜éšæ¨¡å‹ï¼š{error_msg}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.toast(f"ğŸ§  åˆ‡æ›è‡³æ·±åº¦æ€è€ƒæ¨¡å¼ ({model_name})...", icon="ğŸ’¡")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model_smart = genai.GenerativeModel(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model_name=model_name,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  system_instruction=GEM_INSTRUCTIONS,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  generation_config={"temperature": 0.2}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.chat_message("ai"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message_placeholder = st.empty()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_response = ""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_prompt = f"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {st.session_state.last_prompt_content}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  å¯©æ ¸è¡¨åƒè€ƒï¼š
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {current_md}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  è«‹æ­£å¼ç”¢å‡ºã€è©¦é¡Œã€‘èˆ‡ã€åƒè€ƒç­”æ¡ˆå·ã€‘ã€‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = generate_with_retry(model_smart, final_prompt, stream=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for chunk in response:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_response += chunk.text
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message_placeholder.markdown(full_response + "â–Œ")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message_placeholder.markdown(full_response)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append({"role": "model", "content": full_response})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e: st.error(f"å‘½é¡Œå¤±æ•—ï¼š{e}")

Â  Â  Â  Â  with cb2:
Â  Â  Â  Â  Â  Â  if st.button("â¬…ï¸ è¿”å›ä¿®æ”¹åƒæ•¸", use_container_width=True):
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.phase = 1
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history = []
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â Â 
Â  Â  # å¾®èª¿
Â  Â  if len(st.session_state.chat_history) > 1:
Â  Â  Â  Â  if prompt := st.chat_input("å°é¡Œç›®ä¸æ»¿æ„ï¼Ÿè«‹è¼¸å…¥æŒ‡ä»¤å¾®èª¿"):
Â  Â  Â  Â  Â  Â  with st.chat_message("user"): st.markdown(prompt)
Â  Â  Â  Â  Â  Â  with st.spinner("ğŸ”§ AI æ­£åœ¨ä¿®æ”¹è©¦é¡Œ..."):
Â  Â  Â  Â  Â  Â  Â  Â  keys = [k.strip() for k in api_input.replace('\n', ',').split(',') if k.strip()]
Â  Â  Â  Â  Â  Â  Â  Â  genai.configure(api_key=random.choice(keys))
Â  Â  Â  Â  Â  Â  Â  Â  model_pro = genai.GenerativeModel("gemini-1.5-pro", system_instruction=GEM_INSTRUCTIONS)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  history_for_chat = []
Â  Â  Â  Â  Â  Â  Â  Â  history_for_chat.append({"role": "user", "parts": [st.session_state.last_prompt_content]})
Â  Â  Â  Â  Â  Â  Â  Â  history_for_chat.append({"role": "model", "parts": [current_md]})
Â  Â  Â  Â  Â  Â  Â  Â  if len(st.session_state.chat_history) > 1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â history_for_chat.append({"role": "model", "parts": [st.session_state.chat_history[-1]["content"]]})
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  chat_pro = model_pro.start_chat(history=history_for_chat)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with st.chat_message("ai"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message_placeholder = st.empty()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_response = ""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = generate_with_retry(chat_pro, prompt, stream=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for chunk in response:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_response += chunk.text
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message_placeholder.markdown(full_response + "â–Œ")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message_placeholder.markdown(full_response)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append({"role": "model", "content": full_response})

st.markdown('<div class="custom-footer">Â© 2026 æ–°ç«¹å¸‚é¦™å±±å€å…§æ¹–åœ‹å°. All Rights Reserved.</div>', unsafe_allow_html=True)
