import streamlit as st
import google.generativeai as genai
import io
import pandas as pd
import math
from pypdf import PdfReader
from docx import Document
from pptx import Presentation

# --- 1. æ ¸å¿ƒè¨­å®š ---
SUBJECT_Q_TYPES = {
    "åœ‹èª": ["åœ‹å­—æ³¨éŸ³", "æ”¹éŒ¯å­—", "å­—è©ç¾©æ¸¬é©—", "èª²æ–‡ç†è§£", "é–±è®€æ¸¬é©—", "æˆèªé‹ç”¨"],
    "æ•¸å­¸": ["é¸æ“‡é¡Œ", "å¡«å……é¡Œ", "è¨ˆç®—é¡Œ", "æ‡‰ç”¨é¡Œ", "ç•«åœ–é¡Œ"],
    "è‡ªç„¶ç§‘å­¸": ["æ˜¯éé¡Œ", "é¸æ“‡é¡Œ", "åšåšçœ‹", "ç§‘å­¸é–±è®€", "å¯¦é©—é¡Œ"],
    "ç¤¾æœƒ": ["æ˜¯éé¡Œ", "é¸æ“‡é¡Œ", "å‹¾é¸é¡Œ", "é€£é€£çœ‹", "ç°¡ç­”é¡Œ", "åœ–è¡¨é¡Œ"],
    "è‹±èª": ["Listen & Check", "Listen & Choose", "Read & Choose", "Look & Write", "Reading Comprehension"],
    "": ["å–®é¸é¡Œ", "æ˜¯éé¡Œ", "å¡«å……é¡Œ", "ç°¡ç­”é¡Œ"]
}

# --- 2. æª”æ¡ˆè®€å–å·¥å…· ---
@st.cache_data
def extract_text_from_files(files):
    text_content = ""
    for file in files:
        try:
            filename = file.name.lower()
            file_header = f"\n\n=== æª”æ¡ˆä¾†æºï¼š{file.name} ===\n"
            extracted_text = ""

            if filename.endswith('.pdf'):
                pdf_reader = PdfReader(file)
                for page in pdf_reader.pages:
                    extracted_text += (page.extract_text() or "") + "\n"
                if len(extracted_text.strip()) < 10:
                    text_content += file_header + "[è­¦ç¤º] å…§å®¹éå°‘ï¼Œå¯èƒ½æ˜¯æƒææª”ï¼Œè«‹å…ˆè½‰æª”ã€‚\n"
                else:
                    text_content += file_header + extracted_text

            elif filename.endswith('.docx'):
                doc = Document(file)
                extracted_text = "\n".join([p.text for p in doc.paragraphs])
                text_content += file_header + extracted_text

            elif filename.endswith('.pptx'):
                try:
                    prs = Presentation(file)
                    for slide_idx, slide in enumerate(prs.slides):
                        slide_text = []
                        for shape in slide.shapes:
                            if hasattr(shape, "text") and shape.text.strip():
                                slide_text.append(shape.text)
                        if slide_text:
                            extracted_text += f"[Slide {slide_idx+1}]\n" + "\n".join(slide_text) + "\n"
                    text_content += file_header + extracted_text
                except:
                    text_content += file_header + "[PPTX è®€å–éŒ¯èª¤] è«‹ç¢ºèªæª”æ¡ˆæœªåŠ å¯†ã€‚\n"
            
            elif filename.endswith('.txt'):
                text_content += file_header + str(file.read(), "utf-8")
                
        except Exception as e:
            text_content += f"\n[è®€å–éŒ¯èª¤] {str(e)}\n"
    return text_content

# --- 3. é‚è¼¯æ ¸å¿ƒï¼šé˜²å‘†ç®—åˆ†ç³»çµ± ---
def calculate_scores(df):
    # é å…ˆå»ºç«‹å¿…è¦æ¬„ä½ï¼Œé˜²æ­¢ KeyError
    if 'ç›®æ¨™åˆ†é…ç¯€æ•¸' not in df.columns: df['ç›®æ¨™åˆ†é…ç¯€æ•¸'] = 0.0
    if 'é è¨ˆé…åˆ†' not in df.columns: df['é è¨ˆé…åˆ†'] = 0.0

    try:
        # 1. æ¬„ä½åç¨±æ¨™æº–åŒ–
        if 'æˆèª²ç¯€æ•¸' in df.columns:
            df.rename(columns={'æˆèª²ç¯€æ•¸': 'å–®å…ƒç¸½ç¯€æ•¸'}, inplace=True)
        
        # 2. å¼·åˆ¶è½‰æ•¸å€¼ (é—œéµï¼æŠŠ "æœªæä¾›" è®Šæˆ 1)
        df['å–®å…ƒç¸½ç¯€æ•¸'] = pd.to_numeric(df['å–®å…ƒç¸½ç¯€æ•¸'], errors='coerce').fillna(1)
        
        # 3. è¨ˆç®—æ¯å€‹å–®å…ƒæœ‰å¹¾æ¢ç›®æ¨™
        unit_counts = df['å–®å…ƒåç¨±'].value_counts()
        
        # 4. å¹³å‡åˆ†é…ç¯€æ•¸ (å–®å…ƒç¸½ç¯€æ•¸ / ç›®æ¨™æ•¸)
        def distribute_hours(row):
            unit_name = row['å–®å…ƒåç¨±']
            total_unit_hours = row['å–®å…ƒç¸½ç¯€æ•¸']
            count = unit_counts.get(unit_name, 1)
            if count == 0: count = 1
            return total_unit_hours / count

        df['ç›®æ¨™åˆ†é…ç¯€æ•¸'] = df.apply(distribute_hours, axis=1)

        # 5. è¨ˆç®—æ•´ä»½è€ƒå·çš„ç¸½æ¬Šé‡ (é¿å…é‡è¤‡åŠ ç¸½)
        # æˆ‘å€‘åªå–æ¯å€‹å–®å…ƒçš„ç¬¬ä¸€ç­†ä¾†åŠ ç¸½ã€Œå–®å…ƒç¸½ç¯€æ•¸ã€
        unit_hours_map = df[['å–®å…ƒåç¨±', 'å–®å…ƒç¸½ç¯€æ•¸']].drop_duplicates()
        total_course_hours = unit_hours_map['å–®å…ƒç¸½ç¯€æ•¸'].sum()
        if total_course_hours == 0: total_course_hours = 1

        # 6. è¨ˆç®—é…åˆ†
        df['åŸå§‹é…åˆ†'] = (df['ç›®æ¨™åˆ†é…ç¯€æ•¸'] / total_course_hours) * 100
        df['é è¨ˆé…åˆ†'] = df['åŸå§‹é…åˆ†'].apply(lambda x: round(x, 1))

        # 7. å¾®èª¿ç¸½åˆ†è‡³ 100 (ä¿®æ­£å°æ•¸é»èª¤å·®)
        current_sum = df['é è¨ˆé…åˆ†'].sum()
        diff = 100 - current_sum
        if abs(diff) > 0.01: 
            df.iloc[-1, df.columns.get_loc('é è¨ˆé…åˆ†')] += diff

        return df
    except Exception as e:
        st.error(f"âš ï¸ é…åˆ†è¨ˆç®—ç™¼ç”Ÿä¾‹å¤–ç‹€æ³ (å·²è‡ªå‹•ç•¥é): {e}")
        return df

# --- 4. Excel ä¸‹è¼‰ (ä¿®å¾©ç‰ˆ) ---
def df_to_excel(df):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        export_df = df.copy()
        
        # åªåŒ¯å‡ºå­˜åœ¨çš„æ¬„ä½
        desired_cols = ['å–®å…ƒåç¨±', 'å–®å…ƒç¸½ç¯€æ•¸', 'å­¸ç¿’ç›®æ¨™', 'ç›®æ¨™åˆ†é…ç¯€æ•¸', 'é è¨ˆé…åˆ†']
        final_cols = [c for c in desired_cols if c in export_df.columns]
        export_df = export_df[final_cols]
        
        if 'ç›®æ¨™åˆ†é…ç¯€æ•¸' in export_df.columns:
            export_df.rename(columns={'ç›®æ¨™åˆ†é…ç¯€æ•¸': 'æ­¤ç›®æ¨™ä½”ç”¨ç¯€æ•¸'}, inplace=True)
        
        export_df.to_excel(writer, index=False, sheet_name='å­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨')
        workbook = writer.book
        worksheet = writer.sheets['å­¸ç¿’ç›®æ¨™å¯©æ ¸è¡¨']
        
        header_fmt = workbook.add_format({'bold': True, 'align': 'center', 'bg_color': '#DCE6F1', 'border': 1})
        cell_fmt = workbook.add_format({'text_wrap': True, 'valign': 'top', 'border': 1})
        num_fmt = workbook.add_format({'num_format': '0.0', 'border': 1, 'align': 'center'})
        
        # è¨­å®šæ¬„å¯¬
        worksheet.set_column('A:A', 15, cell_fmt) 
        worksheet.set_column('B:B', 10, num_fmt) 
        worksheet.set_column('C:C', 60, cell_fmt) 
        worksheet.set_column('D:D', 12, num_fmt)
        worksheet.set_column('E:E', 12, num_fmt)
        
        for col_num, value in enumerate(export_df.columns.values):
            worksheet.write(0, col_num, value, header_fmt)
            
    return output.getvalue()

# --- 5. æ¨¡å‹é¸æ“‡ ---
def get_available_flash_model(api_key):
    try:
        genai.configure(api_key=api_key)
        valid_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        for m in valid_models:
             if 'flash' in m.lower(): return m
        return "models/gemini-1.5-flash"
    except: return "models/gemini-1.5-flash"

# --- 6. Prompt (é‡å°æ•¸å­—åˆ†é»æ‹†è§£çš„ç‰¹åŒ–ç‰ˆ) ---
GEM_EXTRACT_PROMPT = """
ä½ æ˜¯ä¸€å€‹ç²¾æº–çš„æ•™æåˆ†æå¸«ã€‚è«‹åˆ†æä»¥ä¸‹æ•™æï¼Œæå–ã€Œå–®å…ƒåç¨±ã€ã€ã€Œå­¸ç¿’ç›®æ¨™ã€èˆ‡ã€Œå–®å…ƒç¸½æˆèª²ç¯€æ•¸ã€ã€‚

**âš ï¸ æœ€é«˜æŒ‡ä»¤ï¼šæ•¸å­—æ‹†è§£åŸå‰‡**
1. **çœ‹åˆ°æ•¸å­—åˆ†é» (1., 2., 3...)ï¼Œå¿…é ˆæ‹†æˆä¸åŒåˆ—ï¼**
   - å¦‚æœå–®å…ƒå…§å®¹æœ‰ï¼šã€Œ1. çŸ¥é“... 2. å¯Ÿè¦º... 3. äº†è§£...ã€
   - è«‹å‹™å¿…è¼¸å‡º **3 åˆ—** è³‡æ–™ï¼Œæ¯ä¸€åˆ—åªæ”¾ä¸€å€‹ç›®æ¨™ã€‚
   - **çµ•å°ç¦æ­¢** æŠŠ 1, 2, 3 å¯«åœ¨åŒä¸€æ ¼ã€‚

**è¼¸å‡ºæ ¼å¼è¦å‰‡ï¼š**
1. åƒ…è¼¸å‡ºä¸€å€‹ Markdown è¡¨æ ¼ã€‚
2. æ¬„ä½ï¼š| å–®å…ƒåç¨± | å­¸ç¿’ç›®æ¨™ | æˆèª²ç¯€æ•¸ |
3. **å–®å…ƒåç¨±**ï¼šè‹¥è©²å–®å…ƒæœ‰ 10 å€‹ç›®æ¨™ï¼Œè«‹åœ¨ã€Œå–®å…ƒåç¨±ã€æ¬„ä½é‡è¤‡å¡«å¯« 10 æ¬¡è©²å–®å…ƒçš„åå­—ã€‚
4. **æˆèª²ç¯€æ•¸**ï¼š
   - è«‹å¡«å…¥è©²å–®å…ƒçš„ã€Œç¸½ç¯€æ•¸ã€(æ•¸å­—)ã€‚
   - å¦‚æœæ‰¾ä¸åˆ°ï¼Œè«‹å¡«å…¥ "1"ã€‚
   - **ä¸è¦** å¯«æ–‡å­—ï¼Œåªèƒ½å¯«æ•¸å­—ã€‚

æ•™æå…§å®¹ï¼š
{content}
"""

# --- 7. ä¸»ç¨‹å¼ ---
st.set_page_config(page_title="å…§æ¹–åœ‹å°å‡ºé¡Œç³»çµ± (Pro)", layout="wide")

st.markdown("""<div style="background:#1E293B;padding:15px;text-align:center;color:white;border-radius:10px;">
<h2>å…§æ¹–åœ‹å° AI å‘½é¡Œç³»çµ± (ç›®æ¨™æ‹†è§£ç‰ˆ)</h2></div>""", unsafe_allow_html=True)

if "extracted_data" not in st.session_state: st.session_state.extracted_data = None
if "step" not in st.session_state: st.session_state.step = 1

with st.sidebar:
    st.header("è¨­å®š")
    api_key = st.text_input("API Key", type="password")
    if st.button("é‡ç½®"): 
        st.session_state.extracted_data = None
        st.session_state.step = 1
        st.rerun()
    
    st.divider()
    with st.expander("ğŸ› ï¸ è½‰æª”å·¥å…·ç®±"):
        st.markdown("[Word è½‰æª”](https://cloudconvert.com/doc-to-docx)")
        st.markdown("[PPT è½‰æª”](https://cloudconvert.com/ppt-to-pptx)")
        st.markdown("[PDF è½‰æ–‡å­—](https://www.ilovepdf.com/zh-tw/pdf_to_word)")

if st.session_state.step == 1:
    uploaded_files = st.file_uploader("ä¸Šå‚³æ•™æ", type=["pdf","docx","pptx","txt"], accept_multiple_files=True)
    if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary"):
        if api_key and uploaded_files:
            with st.spinner("AI æ­£åœ¨é€æ¢æ‹†è§£å­¸ç¿’ç›®æ¨™..."):
                try:
                    text = extract_text_from_files(uploaded_files)
                    model_name = get_available_flash_model(api_key)
                    model = genai.GenerativeModel(model_name)
                    res = model.generate_content(GEM_EXTRACT_PROMPT.format(content=text[:40000]))
                    
                    lines = [l.strip() for l in res.text.split('\n') if "|" in l and "---" not in l]
                    data = []
                    for l in lines:
                        row = [c.strip() for c in l.split('|') if c.strip()]
                        if len(row) >= 3: data.append(row[:3])
                    
                    if data:
                        df = pd.DataFrame(data[1:], columns=["å–®å…ƒåç¨±", "å­¸ç¿’ç›®æ¨™", "æˆèª²ç¯€æ•¸"])
                        df.rename(columns={"æˆèª²ç¯€æ•¸": "å–®å…ƒç¸½ç¯€æ•¸"}, inplace=True)
                        
                        df_cal = calculate_scores(df)
                        st.session_state.extracted_data = df_cal
                        st.session_state.step = 2
                        st.rerun()
                    else:
                        st.error("AI æœªåµæ¸¬åˆ°è¡¨æ ¼è³‡æ–™ï¼Œè«‹æª¢æŸ¥æ•™æå…§å®¹æ˜¯å¦æ¸…æ™°ã€‚")
                except Exception as e: st.error(f"ç™¼ç”ŸéŒ¯èª¤: {e}")

elif st.session_state.step == 2:
    st.info("ğŸ’¡ è«‹æª¢æŸ¥ï¼šå¦‚æœ AI æŠ“çš„ç›®æ¨™æ•¸æ­£ç¢ºï¼Œè«‹åœ¨ã€Œå–®å…ƒç¸½ç¯€æ•¸ã€è¼¸å…¥è©²å–®å…ƒçš„ç¸½èª²æ™‚ (å¦‚ 5)ï¼Œç³»çµ±æœƒè‡ªå‹•åˆ†é…æ¬Šé‡ã€‚")
    
    df_curr = st.session_state.extracted_data
    
    edited_df = st.data_editor(
        df_curr,
        column_config={
            "å–®å…ƒåç¨±": st.column_config.TextColumn(disabled=True),
            "å­¸ç¿’ç›®æ¨™": st.column_config.TextColumn(width="large"),
            "å–®å…ƒç¸½ç¯€æ•¸": st.column_config.NumberColumn("å–®å…ƒç¸½ç¯€æ•¸", min_value=1, max_value=50, help="ä¿®æ”¹æ­¤æ•¸å­—ï¼Œè©²å–®å…ƒæ‰€æœ‰ç›®æ¨™çš„é…åˆ†æœƒè‡ªå‹•æ›´æ–°"),
            "ç›®æ¨™åˆ†é…ç¯€æ•¸": st.column_config.NumberColumn("æ­¤ç›®æ¨™ä½”ç”¨", disabled=True, format="%.2f"),
            "é è¨ˆé…åˆ†": st.column_config.NumberColumn("é…åˆ† (%)", disabled=True)
        },
        use_container_width=True,
        num_rows="dynamic"
    )
    
    if not edited_df.equals(df_curr):
        st.session_state.extracted_data = calculate_scores(edited_df)
        st.rerun()

    st.download_button("ä¸‹è¼‰ Excel", df_to_excel(edited_df), "ç´°ç›®å¯©æ ¸è¡¨.xlsx")
    if st.button("é‡æ–°ä¸Šå‚³"): st.session_state.step=1; st.rerun()
